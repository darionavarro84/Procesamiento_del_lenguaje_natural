{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NV8wZ0MTKjv_"
      },
      "source": [
        "<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n",
        "\n",
        "\n",
        "# Procesamiento de lenguaje natural\n",
        "## Bot de consulta abierta y respuestas predeterminadas con DNN + Spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCVZakCzAjGN"
      },
      "source": [
        "### 1 - Instalar dependencias\n",
        "Para poder utilizar Spacy en castellano es necesario agregar la librería \"spacy-stanza\" para lematizar palabras en español."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "kzao7XO9NJAq"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import string\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_ExOb8uvjqK",
        "outputId": "c247c4c3-7813-400b-900c-e6dee8f16720"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-11 19:18:10.971558: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-11 19:18:10.971618: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-11 19:18:10.971654: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-11 19:18:11.979752: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting es-core-news-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.6.0/es_core_news_sm-3.6.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from es-core-news-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.1.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n"
          ]
        }
      ],
      "source": [
        "# descargaremos un pipeline de pre-procesamiento de SpaCy en español\n",
        "!python -m spacy download es_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "t5IDU0Q6hIsp"
      },
      "outputs": [],
      "source": [
        "# cargar pipeline de pre-procesamiento en español\n",
        "import spacy\n",
        "nlp = spacy.load(\"es_core_news_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wF10RjVMBdV"
      },
      "source": [
        "### 2 - Herramientas de preprocesamiento de datos\n",
        "Entre las tareas de procesamiento de texto en español se implementa:\n",
        "- Quitar acentos y caracteres especiales\n",
        "- Quitar números\n",
        "- Quitar símbolos de puntuación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "ZxoD2hEExmuX"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import unicodedata\n",
        "\n",
        "# El preprocesamento en castellano requiere más trabajo\n",
        "\n",
        "# Referencia de regex:\n",
        "# https://docs.python.org/3/library/re.html\n",
        "\n",
        "def preprocess_clean_text(text):\n",
        "    # Antes de preprocesar los datos se pasa a minúsculas todo el texto\n",
        "    text = text.lower()\n",
        "    # sacar tildes de las palabras:\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    # quitar caracteres especiales\n",
        "    pattern = r'[^a-zA-z0-9.,!?/:;\\\"\\'\\s]' # [^ : ningún caracter de todos estos\n",
        "    # (termina eliminando cualquier caracter distinto de los del regex)\n",
        "    text = re.sub(pattern, '', text)\n",
        "    pattern = r'[^a-zA-z.,!?/:;\\\"\\'\\s]' # igual al anterior pero sin cifras numéricas\n",
        "    # quitar números\n",
        "    text = re.sub(pattern, '', text)\n",
        "    # quitar caracteres de puntuación\n",
        "    text = ''.join([c for c in text if c not in string.punctuation])\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilRbn0KfMm2r"
      },
      "source": [
        "### 3 - Diccionario de entrada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "NgIGpjymNEH7"
      },
      "outputs": [],
      "source": [
        "# Dataset en formato JSON que representa las posibles preguntas (patterns)\n",
        "# y las posibles respuestas por categoría (tag)\n",
        "# Los \"patterns\" van a formar el corpus para entrenar el clasificador que clasifica en tags\n",
        "# \"respones\" son las respuestas predeterminadas posibles para cada tag\n",
        "dataset = {\"intents\": [\n",
        "             {\"tag\": \"bienvenida\",\n",
        "              \"patterns\": [\"Hola\", \"¿Cómo estás?\", \"¿Qué tal?\"],\n",
        "              \"responses\": [\"Hola!\", \"Hola, ¿Cómo estás?\"],\n",
        "             },\n",
        "             {\"tag\": \"nombre\",\n",
        "              \"patterns\": [\"¿Cúal es tu nombre?\", \"¿Quién sos?\"],\n",
        "              \"responses\": [\"Mi nombre es RecomiendaBot\", \"Yo soy el Bot de recomendación de películas\"]\n",
        "             },\n",
        "            {\"tag\": \"acción\",\n",
        "              \"patterns\": [\"acción\", \"suspenso\", \"películas\", \"crimenes\", \"policial\"],\n",
        "              \"responses\": [\"Claro , puedo recomendarte peliculas de acción\", \"Puedo recomendarte una película de suspenso\"]\n",
        "             },\n",
        "            {\"tag\": \"comedia\",\n",
        "              \"patterns\": [\"¿Me recomendas una película cómica?\", \"¿Qué película graciosa puedo ver?\"],\n",
        "              \"responses\": [\"Si, aquí te comparto un listado de películas del género comedia <lista>\"]\n",
        "             },\n",
        "            {\"tag\": \"edad\",\n",
        "              \"patterns\": [\"edad\",\"menores\",\"La pelicula es apta para todo público?\", \"¿Cuál es la edad mínima para ver la película?\", \"¿La pueden ver menores?\"],\n",
        "              \"responses\": [\"Las peliculas que te recomiendo son aptas para todo público\"]\n",
        "             },\n",
        "            {\"tag\": \"guerra\",\n",
        "              \"patterns\": [\"guerra\", \"disparos\", \"bomba\",\"invasión\" , \"armas\"],\n",
        "              \"responses\": [\"Aquí te comparto un listado de películas bélicas <link>\"]\n",
        "             },\n",
        "            {\"tag\": \"románticas\",\n",
        "              \"patterns\": [\"Recomendame películas románticas\", \"romance\", \"amor\"],\n",
        "              \"responses\": [\"Aquí tienes una lista de películas románticas\"]\n",
        "             },\n",
        "            {\"tag\": \"puntaje\",\n",
        "              \"patterns\": [ \"rating\", \"puntaje\" , \"calificación\"],\n",
        "              \"responses\": [\"Aquí te paso las películas recomendadas ordenadas por rating\"]\n",
        "             },\n",
        "             {\"tag\": \"despedida\",\n",
        "              \"patterns\": [ \"Chau\", \"Hasta luego!\", \"gracias\"],\n",
        "              \"responses\": [\"Por nada\", \"Adios\"]\n",
        "             }\n",
        "]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19PEDmIDfLRu"
      },
      "source": [
        "### 4 - Preprocesamiento y armado del dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "b3HP8abHNRk3"
      },
      "outputs": [],
      "source": [
        "# Datos que necesitaremos, las palabras o vocabulario\n",
        "words = []\n",
        "classes = []\n",
        "doc_X = []\n",
        "doc_y = []\n",
        "\n",
        "# Por cada intención (intents) debemos tomar los patrones que la caracterizan\n",
        "# a esa intención y transformarla a tokens para almacenar en doc_X\n",
        "\n",
        "# El tag de cada intención se almacena como doc_Y (la clase a predecir)\n",
        "# En `words` vamos a guardar el vocabulario\n",
        "# En `class` las posibles clases o tags\n",
        "\n",
        "for intent in dataset[\"intents\"]:\n",
        "    for pattern in intent[\"patterns\"]:\n",
        "        # trasformar el patron a tokens\n",
        "        tokens = nlp(preprocess_clean_text(pattern))\n",
        "        # lematizar los tokens\n",
        "        for token in tokens:\n",
        "            words.append(token.lemma_)\n",
        "\n",
        "        doc_X.append(pattern)\n",
        "        doc_y.append(intent[\"tag\"])\n",
        "\n",
        "    # Agregar el tag a las clases\n",
        "    if intent[\"tag\"] not in classes:\n",
        "        classes.append(intent[\"tag\"])\n",
        "\n",
        "# Elminar duplicados con \"set\" y ordenar el vocabulario y las clases por orden alfabético\n",
        "words = sorted(set(words))\n",
        "classes = sorted(set(classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Acy-gcugNbMH",
        "outputId": "22b3bfba-f1b5-4c17-85f5-3d911d941b43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "words: ['accion', 'amor', 'apto', 'arma', 'bomba', 'calificacion', 'chau', 'comico', 'como', 'crimen', 'cual', 'dispar', 'edad', 'el', 'este', 'gracias', 'gracioso', 'guerra', 'hasta', 'holar', 'invasion', 'luego', 'menor', 'minimo', 'nombre', 'para', 'pelicula', 'pelicular', 'poder', 'policial', 'publico', 'puntaje', 'que', 'quien', 'rating', 'recomendame', 'recomendas', 'romance', 'romantica', 'ser', 'so', 'suspenso', 'tal', 'todo', 'tu', 'uno', 'ver', 'yo', 'él']\n",
            "classes: ['acción', 'bienvenida', 'comedia', 'despedida', 'edad', 'guerra', 'nombre', 'puntaje', 'románticas']\n",
            "doc_X: ['Hola', '¿Cómo estás?', '¿Qué tal?', '¿Cúal es tu nombre?', '¿Quién sos?', 'acción', 'suspenso', 'películas', 'crimenes', 'policial', '¿Me recomendas una película cómica?', '¿Qué película graciosa puedo ver?', 'edad', 'menores', 'La pelicula es apta para todo público?', '¿Cuál es la edad mínima para ver la película?', '¿La pueden ver menores?', 'guerra', 'disparos', 'bomba', 'invasión', 'armas', 'Recomendame películas románticas', 'romance', 'amor', 'rating', 'puntaje', 'calificación', 'Chau', 'Hasta luego!', 'gracias']\n",
            "doc_y: ['bienvenida', 'bienvenida', 'bienvenida', 'nombre', 'nombre', 'acción', 'acción', 'acción', 'acción', 'acción', 'comedia', 'comedia', 'edad', 'edad', 'edad', 'edad', 'edad', 'guerra', 'guerra', 'guerra', 'guerra', 'guerra', 'románticas', 'románticas', 'románticas', 'puntaje', 'puntaje', 'puntaje', 'despedida', 'despedida', 'despedida']\n"
          ]
        }
      ],
      "source": [
        "print(\"words:\", words)\n",
        "print(\"classes:\", classes)\n",
        "print(\"doc_X:\", doc_X)\n",
        "print(\"doc_y:\", doc_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YI0L2U7IQcvy",
        "outputId": "b1d02b31-e3e2-4a39-ad01-b75f4b052e51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulario: 49\n"
          ]
        }
      ],
      "source": [
        "# Tamaño del vocabulario\n",
        "print(\"Vocabulario:\", len(words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqBeGKRk_q4r",
        "outputId": "419be3fc-0119-427e-d1d2-74a85c345782"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tags: 9\n"
          ]
        }
      ],
      "source": [
        "# Cantidad de tags\n",
        "print(\"Tags:\", len(classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpbJ0guPN2Uq",
        "outputId": "7179f8b8-1854-4ef1-eea3-a117c989bd99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] y: [0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
            "X: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] y: [0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
            "X: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0] y: [0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
            "X: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0] y: [0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
            "X: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0] y: [0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
            "X: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] y: [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "X: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0] y: [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "X: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] y: [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "X: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] y: [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "X: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] y: [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "X: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0] y: [0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
            "X: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0] y: [0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
            "X: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] y: [0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
            "X: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] y: [0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
            "X: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0] y: [0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
            "X: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0] y: [0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
            "X: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1] y: [0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
            "X: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] y: [0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
            "X: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] y: [0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
            "X: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] y: [0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
            "X: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] y: [0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
            "X: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] y: [0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
            "X: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] y: [0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
            "X: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] y: [0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
            "X: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] y: [0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
            "X: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] y: [0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
            "X: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] y: [0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
            "X: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] y: [0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
            "X: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] y: [0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
            "X: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] y: [0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
            "X: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] y: [0, 0, 0, 1, 0, 0, 0, 0, 0]\n"
          ]
        }
      ],
      "source": [
        "# Transformar doc_X en bag of words por oneHotEncoding\n",
        "# Transformar doc_Y en un vector de clases multicategórico con oneHotEncoding\n",
        "\n",
        "training = []\n",
        "out_empty = [0] * len(classes)\n",
        "\n",
        "for idx, doc in enumerate(doc_X):\n",
        "    # Transformar la pregunta (input) en tokens y lematizar\n",
        "    text = []\n",
        "    tokens = nlp(preprocess_clean_text(doc.lower()))\n",
        "    for token in tokens:\n",
        "        text.append(token.lemma_)\n",
        "\n",
        "    # Transformar los tokens en \"Bag of words\" (arrays de 1 y 0)\n",
        "    bow = []\n",
        "    for word in words:\n",
        "\n",
        "\n",
        "        bow.append(1) if word in text else bow.append(0)\n",
        "\n",
        "    # Crear el array de salida (class output) correspondiente\n",
        "    output_row = list(out_empty)\n",
        "    output_row[classes.index(doc_y[idx])] = 1\n",
        "\n",
        "    print(\"X:\", bow, \"y:\", output_row)\n",
        "    training.append([bow, output_row])\n",
        "\n",
        "# Mezclar los datos\n",
        "random.shuffle(training)\n",
        "training = np.array(training, dtype=object)\n",
        "\n",
        "# Dividir en datos de entrada y salida para train\n",
        "train_X = np.array(list(training[:, 0]))\n",
        "train_y = np.array(list(training[:, 1]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_Hr8QaDfRf3"
      },
      "source": [
        "### 5 - Entrenamiento del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fopb3NqcAGTz",
        "outputId": "2d0fbf8b-7d63-4b22-86be-a29926da456e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input: (49,) output: 9\n"
          ]
        }
      ],
      "source": [
        "# Shape de entrada y salida\n",
        "input_shape = (train_X.shape[1],)\n",
        "output_shape = train_y.shape[1]\n",
        "print(\"input:\", input_shape, \"output:\", output_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xy7tzkwdOZx9",
        "outputId": "113d8877-eb04-454a-b530-f901e7c48e92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_45 (Dense)            (None, 128)               6400      \n",
            "                                                                 \n",
            " dropout_30 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_46 (Dense)            (None, 64)                8256      \n",
            "                                                                 \n",
            " dropout_31 (Dropout)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_47 (Dense)            (None, 9)                 585       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 15241 (59.54 KB)\n",
            "Trainable params: 15241 (59.54 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# Entrenamiento del modelo DNN\n",
        "# - Modelo secuencial\n",
        "# - Con regularización\n",
        "# - softmax y optimizador Adam\n",
        "\n",
        "from keras.layers import Dense, Dropout, Embedding, Flatten\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_shape=input_shape, activation=\"relu\"))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation=\"relu\"))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(output_shape, activation = \"softmax\"))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=\"Adam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamiento del modelo DNN\n",
        "# - Modelo secuencial\n",
        "# - Con regularización\n",
        "# - softmax y optimizador Adam\n",
        "#Agrego una capa de embeddings a la entrada\n",
        "\n",
        "from keras.layers import Dense, Dropout, Embedding, Flatten\n",
        "\n",
        "model_emb = Sequential()\n",
        "model_emb.add(Embedding(input_dim=len(words), output_dim=512, input_length=input_shape[0]))\n",
        "model_emb.add(Flatten())\n",
        "model_emb.add(Dense(128, input_shape=input_shape, activation=\"relu\"))\n",
        "model_emb.add(Dropout(0.5))\n",
        "model_emb.add(Dense(64, activation=\"relu\"))\n",
        "model_emb.add(Dropout(0.5))\n",
        "model_emb.add(Dense(output_shape, activation = \"softmax\"))\n",
        "\n",
        "model_emb.compile(loss='categorical_crossentropy',\n",
        "              optimizer=\"Adam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "print(model_emb.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F94Q9C2w79Bv",
        "outputId": "b119a4b1-c6af-43a7-b2d1-3a4aaa2d82ac"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_10 (Embedding)    (None, 49, 512)           25088     \n",
            "                                                                 \n",
            " flatten_10 (Flatten)        (None, 25088)             0         \n",
            "                                                                 \n",
            " dense_48 (Dense)            (None, 128)               3211392   \n",
            "                                                                 \n",
            " dropout_32 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_49 (Dense)            (None, 64)                8256      \n",
            "                                                                 \n",
            " dropout_33 (Dropout)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_50 (Dense)            (None, 9)                 585       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3245321 (12.38 MB)\n",
            "Trainable params: 3245321 (12.38 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6hi4EcdOghm",
        "outputId": "2354f1de-80d4-44df-f6c3-5d2dcb6556d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "1/1 [==============================] - 1s 581ms/step - loss: 2.2324 - accuracy: 0.0968\n",
            "Epoch 2/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.2289 - accuracy: 0.1290\n",
            "Epoch 3/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.2390 - accuracy: 0.1290\n",
            "Epoch 4/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.2317 - accuracy: 0.1290\n",
            "Epoch 5/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.1667 - accuracy: 0.0968\n",
            "Epoch 6/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.2159 - accuracy: 0.0645\n",
            "Epoch 7/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.1857 - accuracy: 0.1290\n",
            "Epoch 8/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.1474 - accuracy: 0.1290\n",
            "Epoch 9/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.2071 - accuracy: 0.1290\n",
            "Epoch 10/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.1293 - accuracy: 0.1290\n",
            "Epoch 11/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.1567 - accuracy: 0.1290\n",
            "Epoch 12/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.1759 - accuracy: 0.1290\n",
            "Epoch 13/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.1121 - accuracy: 0.2903\n",
            "Epoch 14/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.0853 - accuracy: 0.2581\n",
            "Epoch 15/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.1235 - accuracy: 0.2258\n",
            "Epoch 16/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.1040 - accuracy: 0.2258\n",
            "Epoch 17/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.1844 - accuracy: 0.1613\n",
            "Epoch 18/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.1038 - accuracy: 0.2581\n",
            "Epoch 19/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.1092 - accuracy: 0.3226\n",
            "Epoch 20/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.0326 - accuracy: 0.3871\n",
            "Epoch 21/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.0504 - accuracy: 0.2581\n",
            "Epoch 22/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.0653 - accuracy: 0.2258\n",
            "Epoch 23/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.0975 - accuracy: 0.2258\n",
            "Epoch 24/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.0308 - accuracy: 0.3226\n",
            "Epoch 25/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.0861 - accuracy: 0.1613\n",
            "Epoch 26/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.0714 - accuracy: 0.2581\n",
            "Epoch 27/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.0087 - accuracy: 0.2581\n",
            "Epoch 28/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.0210 - accuracy: 0.2581\n",
            "Epoch 29/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.0479 - accuracy: 0.2258\n",
            "Epoch 30/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.9635 - accuracy: 0.3548\n",
            "Epoch 31/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.9897 - accuracy: 0.3226\n",
            "Epoch 32/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.9652 - accuracy: 0.4516\n",
            "Epoch 33/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.9574 - accuracy: 0.3871\n",
            "Epoch 34/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.9689 - accuracy: 0.2581\n",
            "Epoch 35/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.9137 - accuracy: 0.3871\n",
            "Epoch 36/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.9082 - accuracy: 0.3226\n",
            "Epoch 37/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.9328 - accuracy: 0.3871\n",
            "Epoch 38/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.9506 - accuracy: 0.4516\n",
            "Epoch 39/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.9057 - accuracy: 0.2903\n",
            "Epoch 40/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.9019 - accuracy: 0.5161\n",
            "Epoch 41/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.9283 - accuracy: 0.4516\n",
            "Epoch 42/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.9647 - accuracy: 0.3548\n",
            "Epoch 43/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.8537 - accuracy: 0.5161\n",
            "Epoch 44/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.8911 - accuracy: 0.3548\n",
            "Epoch 45/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.8365 - accuracy: 0.5806\n",
            "Epoch 46/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.9030 - accuracy: 0.4839\n",
            "Epoch 47/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.8501 - accuracy: 0.5161\n",
            "Epoch 48/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.8817 - accuracy: 0.3871\n",
            "Epoch 49/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.8774 - accuracy: 0.4194\n",
            "Epoch 50/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.8528 - accuracy: 0.5161\n",
            "Epoch 51/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.8483 - accuracy: 0.5161\n",
            "Epoch 52/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.8064 - accuracy: 0.5484\n",
            "Epoch 53/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.8144 - accuracy: 0.3871\n",
            "Epoch 54/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.8033 - accuracy: 0.5161\n",
            "Epoch 55/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.7823 - accuracy: 0.5484\n",
            "Epoch 56/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.7288 - accuracy: 0.5161\n",
            "Epoch 57/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.7187 - accuracy: 0.5806\n",
            "Epoch 58/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.7910 - accuracy: 0.5484\n",
            "Epoch 59/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.8166 - accuracy: 0.4194\n",
            "Epoch 60/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.7968 - accuracy: 0.5161\n",
            "Epoch 61/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.7224 - accuracy: 0.5161\n",
            "Epoch 62/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.7175 - accuracy: 0.5806\n",
            "Epoch 63/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.7009 - accuracy: 0.4516\n",
            "Epoch 64/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.7916 - accuracy: 0.3871\n",
            "Epoch 65/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.6419 - accuracy: 0.5161\n",
            "Epoch 66/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.6103 - accuracy: 0.6129\n",
            "Epoch 67/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.7044 - accuracy: 0.5161\n",
            "Epoch 68/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.6198 - accuracy: 0.5806\n",
            "Epoch 69/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.6463 - accuracy: 0.5806\n",
            "Epoch 70/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.5800 - accuracy: 0.6129\n",
            "Epoch 71/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.6016 - accuracy: 0.6129\n",
            "Epoch 72/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.6093 - accuracy: 0.5484\n",
            "Epoch 73/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.5841 - accuracy: 0.5161\n",
            "Epoch 74/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.6036 - accuracy: 0.6129\n",
            "Epoch 75/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.5830 - accuracy: 0.5484\n",
            "Epoch 76/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.5625 - accuracy: 0.5806\n",
            "Epoch 77/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.4726 - accuracy: 0.7097\n",
            "Epoch 78/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.5080 - accuracy: 0.5484\n",
            "Epoch 79/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.4493 - accuracy: 0.6452\n",
            "Epoch 80/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.4490 - accuracy: 0.6452\n",
            "Epoch 81/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.5259 - accuracy: 0.5806\n",
            "Epoch 82/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.5788 - accuracy: 0.6129\n",
            "Epoch 83/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.3979 - accuracy: 0.6129\n",
            "Epoch 84/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.4883 - accuracy: 0.6129\n",
            "Epoch 85/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.4050 - accuracy: 0.6774\n",
            "Epoch 86/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.5872 - accuracy: 0.5161\n",
            "Epoch 87/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.4042 - accuracy: 0.7097\n",
            "Epoch 88/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.3575 - accuracy: 0.6774\n",
            "Epoch 89/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.4093 - accuracy: 0.6774\n",
            "Epoch 90/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.5309 - accuracy: 0.4839\n",
            "Epoch 91/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.3871 - accuracy: 0.7419\n",
            "Epoch 92/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.4118 - accuracy: 0.5806\n",
            "Epoch 93/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.3806 - accuracy: 0.7419\n",
            "Epoch 94/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.4211 - accuracy: 0.6452\n",
            "Epoch 95/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.3377 - accuracy: 0.6129\n",
            "Epoch 96/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.4006 - accuracy: 0.6452\n",
            "Epoch 97/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.3364 - accuracy: 0.8065\n",
            "Epoch 98/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.3689 - accuracy: 0.7097\n",
            "Epoch 99/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.4285 - accuracy: 0.5484\n",
            "Epoch 100/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.2774 - accuracy: 0.7742\n",
            "Epoch 101/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.3814 - accuracy: 0.6774\n",
            "Epoch 102/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.2702 - accuracy: 0.6774\n",
            "Epoch 103/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.2180 - accuracy: 0.6774\n",
            "Epoch 104/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.2476 - accuracy: 0.7419\n",
            "Epoch 105/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.2131 - accuracy: 0.7097\n",
            "Epoch 106/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.0941 - accuracy: 0.8387\n",
            "Epoch 107/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.2685 - accuracy: 0.6452\n",
            "Epoch 108/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.3033 - accuracy: 0.7419\n",
            "Epoch 109/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.2974 - accuracy: 0.7097\n",
            "Epoch 110/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.1890 - accuracy: 0.7419\n",
            "Epoch 111/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.1630 - accuracy: 0.7097\n",
            "Epoch 112/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.1212 - accuracy: 0.8710\n",
            "Epoch 113/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.1534 - accuracy: 0.7419\n",
            "Epoch 114/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.9753 - accuracy: 0.8387\n",
            "Epoch 115/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.0372 - accuracy: 0.7742\n",
            "Epoch 116/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.1411 - accuracy: 0.7742\n",
            "Epoch 117/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.0324 - accuracy: 0.8387\n",
            "Epoch 118/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.0288 - accuracy: 0.8387\n",
            "Epoch 119/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.1227 - accuracy: 0.6774\n",
            "Epoch 120/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.8859 - accuracy: 0.9032\n",
            "Epoch 121/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.9702 - accuracy: 0.8387\n",
            "Epoch 122/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.0351 - accuracy: 0.8065\n",
            "Epoch 123/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.9988 - accuracy: 0.8065\n",
            "Epoch 124/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.9436 - accuracy: 0.8710\n",
            "Epoch 125/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.9220 - accuracy: 0.9032\n",
            "Epoch 126/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.9200 - accuracy: 0.8387\n",
            "Epoch 127/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.9079 - accuracy: 0.8065\n",
            "Epoch 128/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.9533 - accuracy: 0.8387\n",
            "Epoch 129/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.9310 - accuracy: 0.8065\n",
            "Epoch 130/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.9593 - accuracy: 0.8065\n",
            "Epoch 131/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.7725 - accuracy: 0.9355\n",
            "Epoch 132/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.7768 - accuracy: 0.8710\n",
            "Epoch 133/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.8324 - accuracy: 0.8387\n",
            "Epoch 134/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.8612 - accuracy: 0.8065\n",
            "Epoch 135/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.8078 - accuracy: 0.8387\n",
            "Epoch 136/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.8502 - accuracy: 0.8710\n",
            "Epoch 137/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.7355 - accuracy: 0.9032\n",
            "Epoch 138/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.7665 - accuracy: 0.8710\n",
            "Epoch 139/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.9204 - accuracy: 0.8387\n",
            "Epoch 140/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.8229 - accuracy: 0.8710\n",
            "Epoch 141/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6840 - accuracy: 0.9355\n",
            "Epoch 142/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.5921 - accuracy: 0.9355\n",
            "Epoch 143/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.7702 - accuracy: 0.8710\n",
            "Epoch 144/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5969 - accuracy: 0.9032\n",
            "Epoch 145/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6805 - accuracy: 0.8710\n",
            "Epoch 146/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.7571 - accuracy: 0.7419\n",
            "Epoch 147/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6496 - accuracy: 0.9355\n",
            "Epoch 148/200\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.7445 - accuracy: 0.8387\n",
            "Epoch 149/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.7748 - accuracy: 0.8710\n",
            "Epoch 150/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.7300 - accuracy: 0.9355\n",
            "Epoch 151/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6053 - accuracy: 0.9677\n",
            "Epoch 152/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6764 - accuracy: 0.8710\n",
            "Epoch 153/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5996 - accuracy: 0.9677\n",
            "Epoch 154/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.5324 - accuracy: 0.9677\n",
            "Epoch 155/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6490 - accuracy: 0.9032\n",
            "Epoch 156/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.6143 - accuracy: 0.9355\n",
            "Epoch 157/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.5445 - accuracy: 0.9677\n",
            "Epoch 158/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.4769 - accuracy: 1.0000\n",
            "Epoch 159/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.6969 - accuracy: 0.8710\n",
            "Epoch 160/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.4713 - accuracy: 0.9355\n",
            "Epoch 161/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6690 - accuracy: 0.8387\n",
            "Epoch 162/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6115 - accuracy: 0.9032\n",
            "Epoch 163/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5685 - accuracy: 0.9355\n",
            "Epoch 164/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.4998 - accuracy: 0.9677\n",
            "Epoch 165/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.4821 - accuracy: 0.9355\n",
            "Epoch 166/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.5926 - accuracy: 0.9677\n",
            "Epoch 167/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.3570 - accuracy: 1.0000\n",
            "Epoch 168/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.4028 - accuracy: 0.9677\n",
            "Epoch 169/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.3320 - accuracy: 1.0000\n",
            "Epoch 170/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.4668 - accuracy: 0.9032\n",
            "Epoch 171/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.4407 - accuracy: 0.9677\n",
            "Epoch 172/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5500 - accuracy: 0.9677\n",
            "Epoch 173/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.3520 - accuracy: 0.9355\n",
            "Epoch 174/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2901 - accuracy: 0.9677\n",
            "Epoch 175/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.4318 - accuracy: 0.9355\n",
            "Epoch 176/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5192 - accuracy: 0.9032\n",
            "Epoch 177/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.4320 - accuracy: 0.9677\n",
            "Epoch 178/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.4163 - accuracy: 0.9032\n",
            "Epoch 179/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.4327 - accuracy: 0.9677\n",
            "Epoch 180/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.4643 - accuracy: 0.9355\n",
            "Epoch 181/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.3504 - accuracy: 0.9355\n",
            "Epoch 182/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.4549 - accuracy: 0.9032\n",
            "Epoch 183/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.3261 - accuracy: 0.9677\n",
            "Epoch 184/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.3500 - accuracy: 0.9032\n",
            "Epoch 185/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.4774 - accuracy: 0.9355\n",
            "Epoch 186/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2398 - accuracy: 1.0000\n",
            "Epoch 187/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.3583 - accuracy: 1.0000\n",
            "Epoch 188/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.3238 - accuracy: 1.0000\n",
            "Epoch 189/200\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.4411 - accuracy: 0.8710\n",
            "Epoch 190/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.4167 - accuracy: 0.9677\n",
            "Epoch 191/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.3433 - accuracy: 0.9677\n",
            "Epoch 192/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5119 - accuracy: 0.9355\n",
            "Epoch 193/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.4119 - accuracy: 0.8710\n",
            "Epoch 194/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.3586 - accuracy: 0.9677\n",
            "Epoch 195/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.3013 - accuracy: 0.9677\n",
            "Epoch 196/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.4220 - accuracy: 0.9355\n",
            "Epoch 197/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.3955 - accuracy: 0.9677\n",
            "Epoch 198/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.3197 - accuracy: 0.9677\n",
            "Epoch 199/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.3347 - accuracy: 0.9032\n",
            "Epoch 200/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.3395 - accuracy: 0.9032\n"
          ]
        }
      ],
      "source": [
        "#Entreno el modelo sin embeddings\n",
        "hist = model.fit(x=train_X, y=train_y, epochs=200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "Pb1GZDjGRP6Q",
        "outputId": "e1704366-e838-411d-f496-4dc99a07f121"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Entrenamiento\n",
        "epoch_count = range(1, len(hist.history['accuracy']) + 1)\n",
        "sns.lineplot(x=epoch_count,  y=hist.history['accuracy'], label='train')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Entreno el modelo con embeddings\n",
        "hist = model_emb.fit(x=train_X, y=train_y, epochs=200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jj9lxknY8UFA",
        "outputId": "24605036-925f-41b6-fdb1-968f5a5ed894"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "1/1 [==============================] - 1s 666ms/step - loss: 2.1984 - accuracy: 0.0645\n",
            "Epoch 2/200\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 2.6595 - accuracy: 0.0645\n",
            "Epoch 3/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 2.3532 - accuracy: 0.2258\n",
            "Epoch 4/200\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 2.3168 - accuracy: 0.0968\n",
            "Epoch 5/200\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 2.2207 - accuracy: 0.1613\n",
            "Epoch 6/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 2.2507 - accuracy: 0.0968\n",
            "Epoch 7/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 2.2110 - accuracy: 0.2258\n",
            "Epoch 8/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 2.1551 - accuracy: 0.0645\n",
            "Epoch 9/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 2.2634 - accuracy: 0.1290\n",
            "Epoch 10/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 2.1819 - accuracy: 0.1290\n",
            "Epoch 11/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 2.2084 - accuracy: 0.0968\n",
            "Epoch 12/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 2.2036 - accuracy: 0.0645\n",
            "Epoch 13/200\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 2.1801 - accuracy: 0.2258\n",
            "Epoch 14/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 2.1930 - accuracy: 0.1935\n",
            "Epoch 15/200\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 2.0545 - accuracy: 0.1935\n",
            "Epoch 16/200\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 2.1258 - accuracy: 0.1290\n",
            "Epoch 17/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 2.1123 - accuracy: 0.1613\n",
            "Epoch 18/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 2.1282 - accuracy: 0.2581\n",
            "Epoch 19/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 2.1356 - accuracy: 0.2903\n",
            "Epoch 20/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 1.9752 - accuracy: 0.3548\n",
            "Epoch 21/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 2.0972 - accuracy: 0.0968\n",
            "Epoch 22/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 2.1027 - accuracy: 0.1935\n",
            "Epoch 23/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 2.0147 - accuracy: 0.2903\n",
            "Epoch 24/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 2.1258 - accuracy: 0.1290\n",
            "Epoch 25/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 2.0673 - accuracy: 0.1613\n",
            "Epoch 26/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 2.0105 - accuracy: 0.3548\n",
            "Epoch 27/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 1.9992 - accuracy: 0.2581\n",
            "Epoch 28/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 2.0028 - accuracy: 0.3226\n",
            "Epoch 29/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 1.9977 - accuracy: 0.1613\n",
            "Epoch 30/200\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 1.9615 - accuracy: 0.3548\n",
            "Epoch 31/200\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 1.9927 - accuracy: 0.2258\n",
            "Epoch 32/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.9728 - accuracy: 0.2581\n",
            "Epoch 33/200\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 2.0361 - accuracy: 0.2258\n",
            "Epoch 34/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 1.9180 - accuracy: 0.3226\n",
            "Epoch 35/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 1.9097 - accuracy: 0.3871\n",
            "Epoch 36/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 1.8059 - accuracy: 0.3548\n",
            "Epoch 37/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 1.8579 - accuracy: 0.3871\n",
            "Epoch 38/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 1.8302 - accuracy: 0.3548\n",
            "Epoch 39/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 1.9195 - accuracy: 0.2903\n",
            "Epoch 40/200\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 1.9365 - accuracy: 0.2581\n",
            "Epoch 41/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 1.7034 - accuracy: 0.3871\n",
            "Epoch 42/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 1.8962 - accuracy: 0.2258\n",
            "Epoch 43/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 1.8099 - accuracy: 0.3548\n",
            "Epoch 44/200\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 1.7742 - accuracy: 0.3871\n",
            "Epoch 45/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 1.7334 - accuracy: 0.3871\n",
            "Epoch 46/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 1.7053 - accuracy: 0.3548\n",
            "Epoch 47/200\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 1.7357 - accuracy: 0.3871\n",
            "Epoch 48/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 1.7067 - accuracy: 0.4516\n",
            "Epoch 49/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.7194 - accuracy: 0.2258\n",
            "Epoch 50/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.8174 - accuracy: 0.2581\n",
            "Epoch 51/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 1.6193 - accuracy: 0.4839\n",
            "Epoch 52/200\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 1.6437 - accuracy: 0.5161\n",
            "Epoch 53/200\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 1.5842 - accuracy: 0.3871\n",
            "Epoch 54/200\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 1.7138 - accuracy: 0.3871\n",
            "Epoch 55/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 1.6302 - accuracy: 0.3548\n",
            "Epoch 56/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 1.5543 - accuracy: 0.5484\n",
            "Epoch 57/200\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 1.5371 - accuracy: 0.3548\n",
            "Epoch 58/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 1.5572 - accuracy: 0.5161\n",
            "Epoch 59/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 1.4765 - accuracy: 0.4516\n",
            "Epoch 60/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 1.5256 - accuracy: 0.4194\n",
            "Epoch 61/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 1.2769 - accuracy: 0.6452\n",
            "Epoch 62/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 1.4359 - accuracy: 0.4194\n",
            "Epoch 63/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 1.3000 - accuracy: 0.5484\n",
            "Epoch 64/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 1.5746 - accuracy: 0.4516\n",
            "Epoch 65/200\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 1.1964 - accuracy: 0.5806\n",
            "Epoch 66/200\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 1.3107 - accuracy: 0.5806\n",
            "Epoch 67/200\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 1.5519 - accuracy: 0.4516\n",
            "Epoch 68/200\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 1.5023 - accuracy: 0.3226\n",
            "Epoch 69/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 1.1948 - accuracy: 0.6452\n",
            "Epoch 70/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 1.1374 - accuracy: 0.6452\n",
            "Epoch 71/200\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.9581 - accuracy: 0.6129\n",
            "Epoch 72/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 1.2150 - accuracy: 0.5806\n",
            "Epoch 73/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 1.2927 - accuracy: 0.5161\n",
            "Epoch 74/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 1.1667 - accuracy: 0.7097\n",
            "Epoch 75/200\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 1.1247 - accuracy: 0.5484\n",
            "Epoch 76/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 1.1614 - accuracy: 0.6129\n",
            "Epoch 77/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 1.1503 - accuracy: 0.5806\n",
            "Epoch 78/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 1.0224 - accuracy: 0.6129\n",
            "Epoch 79/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.9827 - accuracy: 0.5806\n",
            "Epoch 80/200\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.8781 - accuracy: 0.7742\n",
            "Epoch 81/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 1.1798 - accuracy: 0.6452\n",
            "Epoch 82/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.9807 - accuracy: 0.7742\n",
            "Epoch 83/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.9772 - accuracy: 0.6129\n",
            "Epoch 84/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.8465 - accuracy: 0.7742\n",
            "Epoch 85/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.9366 - accuracy: 0.7097\n",
            "Epoch 86/200\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.8223 - accuracy: 0.6452\n",
            "Epoch 87/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.9510 - accuracy: 0.6129\n",
            "Epoch 88/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.6663 - accuracy: 0.8387\n",
            "Epoch 89/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.7137 - accuracy: 0.8710\n",
            "Epoch 90/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.7566 - accuracy: 0.8065\n",
            "Epoch 91/200\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.8162 - accuracy: 0.7419\n",
            "Epoch 92/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.8353 - accuracy: 0.6774\n",
            "Epoch 93/200\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.7879 - accuracy: 0.6452\n",
            "Epoch 94/200\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.9466 - accuracy: 0.6452\n",
            "Epoch 95/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.7048 - accuracy: 0.7742\n",
            "Epoch 96/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.7477 - accuracy: 0.7742\n",
            "Epoch 97/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 1.0065 - accuracy: 0.6774\n",
            "Epoch 98/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.6949 - accuracy: 0.7742\n",
            "Epoch 99/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.5318 - accuracy: 0.8387\n",
            "Epoch 100/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.6743 - accuracy: 0.8065\n",
            "Epoch 101/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.6630 - accuracy: 0.7419\n",
            "Epoch 102/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.4963 - accuracy: 0.8710\n",
            "Epoch 103/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.7470 - accuracy: 0.7742\n",
            "Epoch 104/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.5495 - accuracy: 0.8387\n",
            "Epoch 105/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.4920 - accuracy: 0.8065\n",
            "Epoch 106/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.4819 - accuracy: 0.8710\n",
            "Epoch 107/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.4181 - accuracy: 0.9032\n",
            "Epoch 108/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.5572 - accuracy: 0.8710\n",
            "Epoch 109/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.6539 - accuracy: 0.7742\n",
            "Epoch 110/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.5693 - accuracy: 0.7419\n",
            "Epoch 111/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.5848 - accuracy: 0.8065\n",
            "Epoch 112/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.3760 - accuracy: 0.9355\n",
            "Epoch 113/200\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.6803 - accuracy: 0.7742\n",
            "Epoch 114/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.3732 - accuracy: 0.9032\n",
            "Epoch 115/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.5920 - accuracy: 0.8710\n",
            "Epoch 116/200\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.4539 - accuracy: 0.8710\n",
            "Epoch 117/200\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.8161 - accuracy: 0.7419\n",
            "Epoch 118/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.2545 - accuracy: 0.9355\n",
            "Epoch 119/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.5193 - accuracy: 0.8710\n",
            "Epoch 120/200\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.4332 - accuracy: 0.8710\n",
            "Epoch 121/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.6970 - accuracy: 0.7097\n",
            "Epoch 122/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.4436 - accuracy: 0.8065\n",
            "Epoch 123/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.4578 - accuracy: 0.7742\n",
            "Epoch 124/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.7109 - accuracy: 0.7097\n",
            "Epoch 125/200\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.3883 - accuracy: 0.8387\n",
            "Epoch 126/200\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.4407 - accuracy: 0.7742\n",
            "Epoch 127/200\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.2726 - accuracy: 0.9032\n",
            "Epoch 128/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.2113 - accuracy: 0.9355\n",
            "Epoch 129/200\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2897 - accuracy: 0.9677\n",
            "Epoch 130/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.6157 - accuracy: 0.7097\n",
            "Epoch 131/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.2185 - accuracy: 0.9355\n",
            "Epoch 132/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.1843 - accuracy: 0.9677\n",
            "Epoch 133/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.6308 - accuracy: 0.7419\n",
            "Epoch 134/200\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.2849 - accuracy: 0.9032\n",
            "Epoch 135/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.2788 - accuracy: 0.9032\n",
            "Epoch 136/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.3136 - accuracy: 0.9355\n",
            "Epoch 137/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.4375 - accuracy: 0.8387\n",
            "Epoch 138/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.2786 - accuracy: 0.9032\n",
            "Epoch 139/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.1148 - accuracy: 1.0000\n",
            "Epoch 140/200\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.3096 - accuracy: 0.9032\n",
            "Epoch 141/200\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.1926 - accuracy: 0.8710\n",
            "Epoch 142/200\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.2679 - accuracy: 0.9355\n",
            "Epoch 143/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.3818 - accuracy: 0.8387\n",
            "Epoch 144/200\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.2354 - accuracy: 0.9355\n",
            "Epoch 145/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.3294 - accuracy: 0.8710\n",
            "Epoch 146/200\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.3517 - accuracy: 0.8387\n",
            "Epoch 147/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.5310 - accuracy: 0.7419\n",
            "Epoch 148/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.2476 - accuracy: 0.9032\n",
            "Epoch 149/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.7065 - accuracy: 0.6774\n",
            "Epoch 150/200\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.2778 - accuracy: 0.9032\n",
            "Epoch 151/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.5045 - accuracy: 0.7742\n",
            "Epoch 152/200\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.3727 - accuracy: 0.9032\n",
            "Epoch 153/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.3211 - accuracy: 0.9032\n",
            "Epoch 154/200\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.2334 - accuracy: 0.9355\n",
            "Epoch 155/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.4001 - accuracy: 0.8387\n",
            "Epoch 156/200\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.5368 - accuracy: 0.8065\n",
            "Epoch 157/200\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.3536 - accuracy: 0.9032\n",
            "Epoch 158/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.3043 - accuracy: 0.9032\n",
            "Epoch 159/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.2146 - accuracy: 0.9032\n",
            "Epoch 160/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.1920 - accuracy: 0.9355\n",
            "Epoch 161/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.3423 - accuracy: 0.9032\n",
            "Epoch 162/200\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.4130 - accuracy: 0.8710\n",
            "Epoch 163/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.3991 - accuracy: 0.8065\n",
            "Epoch 164/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.1442 - accuracy: 0.9677\n",
            "Epoch 165/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.1866 - accuracy: 0.9677\n",
            "Epoch 166/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.1951 - accuracy: 0.9355\n",
            "Epoch 167/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.1737 - accuracy: 0.9032\n",
            "Epoch 168/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.2827 - accuracy: 0.8710\n",
            "Epoch 169/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.4999 - accuracy: 0.8387\n",
            "Epoch 170/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.1527 - accuracy: 0.9677\n",
            "Epoch 171/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.2515 - accuracy: 0.9355\n",
            "Epoch 172/200\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.2886 - accuracy: 0.8710\n",
            "Epoch 173/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.3177 - accuracy: 0.8710\n",
            "Epoch 174/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.3596 - accuracy: 0.9032\n",
            "Epoch 175/200\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.2549 - accuracy: 0.9355\n",
            "Epoch 176/200\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.5996 - accuracy: 0.8065\n",
            "Epoch 177/200\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.3325 - accuracy: 0.9032\n",
            "Epoch 178/200\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.1982 - accuracy: 0.9355\n",
            "Epoch 179/200\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.1952 - accuracy: 0.9677\n",
            "Epoch 180/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.4143 - accuracy: 0.8065\n",
            "Epoch 181/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.2627 - accuracy: 0.8710\n",
            "Epoch 182/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.1765 - accuracy: 0.9032\n",
            "Epoch 183/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.2471 - accuracy: 0.9355\n",
            "Epoch 184/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.2469 - accuracy: 0.9032\n",
            "Epoch 185/200\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.4522 - accuracy: 0.8065\n",
            "Epoch 186/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.5070 - accuracy: 0.8387\n",
            "Epoch 187/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.1215 - accuracy: 1.0000\n",
            "Epoch 188/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.2156 - accuracy: 0.9032\n",
            "Epoch 189/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.1392 - accuracy: 0.9355\n",
            "Epoch 190/200\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.2947 - accuracy: 0.8387\n",
            "Epoch 191/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.3807 - accuracy: 0.8065\n",
            "Epoch 192/200\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.1471 - accuracy: 1.0000\n",
            "Epoch 193/200\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.2799 - accuracy: 0.9032\n",
            "Epoch 194/200\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.2404 - accuracy: 0.9032\n",
            "Epoch 195/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.2766 - accuracy: 0.8387\n",
            "Epoch 196/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.1121 - accuracy: 0.9677\n",
            "Epoch 197/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.2696 - accuracy: 0.8710\n",
            "Epoch 198/200\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2335 - accuracy: 0.9032\n",
            "Epoch 199/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.3554 - accuracy: 0.8710\n",
            "Epoch 200/200\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.1726 - accuracy: 0.9677\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTVDnrV0mDRf",
        "outputId": "55055dc8-67bf-4f75-cf6a-652465f31905"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "# Guardar lo necesario para poder re-utilizar este modelo en el futuro\n",
        "# En NLP además de los modelos es necesario proveer: tokenizador y vocabulario.\n",
        "# La tríada modelo+tokenizador+vocabulario es necesaria para hacer inferencia.\n",
        "# Sin una de las tres no se puede hacer inferencia.\n",
        "\n",
        "# en este caso guardamos:\n",
        "# el vocabulario utilizado (words)\n",
        "# las posibles clases\n",
        "# el modelo\n",
        "import pickle\n",
        "pickle.dump(words, open('words.pkl','wb'))\n",
        "pickle.dump(classes, open('classes.pkl','wb'))\n",
        "#Guardo el modelo sin embedding\n",
        "model.save('chatbot_model.h5')\n",
        "\n",
        "#Guardo el modelo con embedding\n",
        "model_emb.save('chatbot_model_emb.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnD1WvhBfVYR"
      },
      "source": [
        "### 6 - Testing y validación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "kqBdSGt8Orkm"
      },
      "outputs": [],
      "source": [
        "# convertir texto de entrada del usuario a tokens\n",
        "def text_to_tokens(text):\n",
        "    lemma_tokens = []\n",
        "    tokens = nlp(preprocess_clean_text(text))\n",
        "    for token in tokens:\n",
        "        lemma_tokens.append(token.lemma_)\n",
        "    #print(lemma_tokens)\n",
        "    return lemma_tokens\n",
        "\n",
        "# transformar el texto de entrada tokenizado a una representación OHE\n",
        "def bag_of_words(text, vocab):\n",
        "    tokens = text_to_tokens(text)\n",
        "    bow = [0] * len(vocab)\n",
        "    for w in tokens:\n",
        "        for idx, word in enumerate(vocab):\n",
        "            if word == w:\n",
        "                bow[idx] = 1\n",
        "    #print(bow)\n",
        "    return np.array(bow)\n",
        "\n",
        "# usar modelo con la entrada en OHE y los labels posibles (tags)\n",
        "def pred_class(text, vocab, labels):\n",
        "    bow = bag_of_words(text, vocab)\n",
        "    words_recognized = sum(bow)\n",
        "\n",
        "    return_list = []\n",
        "    if words_recognized > 0: # sólo si reconoció alguna palabra del vocabulario\n",
        "        result = model.predict(np.array([bow]))[0] # es un array de softmax\n",
        "        thresh = 0.4\n",
        "        # filtrar aquellas entradas menores al umbral `thresh`\n",
        "        y_pred = [[idx, res] for idx, res in enumerate(result) if res > thresh]\n",
        "        # ordenar keys de acuerdo al valor softmax\n",
        "        y_pred.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # return_list es una lista de los labels de mayor a menor\n",
        "        for r in y_pred:\n",
        "            return_list.append(labels[r[0]])\n",
        "            #print(labels[r[0]], r[1])\n",
        "\n",
        "    # si no reconoció palabras del vocabulario se devuelve una lista vacía\n",
        "    return return_list\n",
        "\n",
        "def pred_class_emb(text, vocab, labels):\n",
        "    bow = bag_of_words(text, vocab)\n",
        "    words_recognized = sum(bow)\n",
        "\n",
        "    return_list = []\n",
        "    if words_recognized > 0: # sólo si reconoció alguna palabra del vocabulario\n",
        "        result = model_emb.predict(np.array([bow]))[0] # es un array de softmax\n",
        "        thresh = 0.4\n",
        "        # filtrar aquellas entradas menores al umbral `thresh`\n",
        "        y_pred_emb = [[idx, res] for idx, res in enumerate(result) if res > thresh]\n",
        "        # ordenar keys de acuerdo al valor softmax\n",
        "        y_pred_emb.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # return_list es una lista de los labels de mayor a menor\n",
        "        for r in y_pred_emb:\n",
        "            return_list.append(labels[r[0]])\n",
        "            #print(labels[r[0]], r[1])\n",
        "\n",
        "    # si no reconoció palabras del vocabulario se devuelve una lista vacía\n",
        "    return return_list\n",
        "\n",
        "\n",
        "# obtener una respuesta predeterminada\n",
        "def get_response(intents_list, intents_json):\n",
        "    tag = intents_list[0] # tomar el tag con el mejor valor softmax\n",
        "    list_of_intents = intents_json[\"intents\"] # intents_json es todo el dataset\n",
        "    for i in list_of_intents:\n",
        "        if i[\"tag\"] == tag: # buscar el tag correspoindiente y dar una respuesta predeterminada aleatoria\n",
        "            result = random.choice(i[\"responses\"])\n",
        "            break\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VhvBqYMU-hKi"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 991
        },
        "id": "Xp1vXQwdOvl7",
        "outputId": "7d2a5643-7c03-4fd4-ff1b-2ba9867c68aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tu identidad?\n",
            "Q: tu identidad?\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "BOT: Perdón, no comprendo la pregunta.\n",
            "Cual es tu identidad?\n",
            "Q: Cual es tu identidad?\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "BOT: Yo soy el Bot de recomendación de películas\n",
            "Recomendame peliculas sobre enamoramientos\n",
            "Q: Recomendame peliculas sobre enamoramientos\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "BOT: Aquí tienes una lista de películas románticas\n",
            "y alguna pelicula sobre investigación de un hecho delictivo?\n",
            "Q: y alguna pelicula sobre investigación de un hecho delictivo?\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "BOT: Puedo recomendarte una película de suspenso\n",
            "Y una pelicula sobre conflictos armados entre naciones?\n",
            "Q: Y una pelicula sobre conflictos armados entre naciones?\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "BOT: Puedo recomendarte una película de suspenso\n",
            "BOT: Puedo recomendarte una película de suspenso No, una película sobre conflictos armados\n",
            "Q: BOT: Puedo recomendarte una película de suspenso No, una película sobre conflictos armados\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "BOT: Claro , puedo recomendarte peliculas de acción\n",
            "Sobre guerras\n",
            "Q: Sobre guerras\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "BOT: Aquí te comparto un listado de películas bélicas <link>\n",
            "Para todas las edades?\n",
            "Q: Para todas las edades?\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "BOT: Las peliculas que te recomiendo son aptas para todo público\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-137-15ff5ef8d1ad>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# pedir input al usuario\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Q:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "#Testeo bot de RN sin embedding\n",
        "while True:\n",
        "    # pedir input al usuario\n",
        "    message = input(\"\")\n",
        "    print(\"Q:\", message)\n",
        "\n",
        "    intents = pred_class(message, words, classes)\n",
        "    if len(intents) > 0:\n",
        "        result = get_response(intents, dataset)\n",
        "        print(\"BOT:\", result)\n",
        "    else: # si no hubo ningún resultado que supere el umbral\n",
        "        print(\"BOT: Perdón, no comprendo la pregunta.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Testeo bot de RN con embedding\n",
        "while True:\n",
        "    # pedir input al usuario\n",
        "    message = input(\"\")\n",
        "    print(\"Q:\", message)\n",
        "\n",
        "    intents = pred_class_emb(message, words, classes)\n",
        "    if len(intents) > 0:\n",
        "        result = get_response(intents, dataset)\n",
        "        print(\"BOT:\", result)\n",
        "    else: # si no hubo ningún resultado que supere el umbral\n",
        "        print(\"BOT: Perdón, no comprendo la pregunta.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 918
        },
        "id": "U7S2n7zn8sAp",
        "outputId": "3319ff6d-a115-40b9-9140-05f2395af56f"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tu identidad?\n",
            "Q: tu identidad?\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "BOT: Yo soy el Bot de recomendación de películas\n",
            "Recomendame peliculas sobre enamoramientos\n",
            "Q: Recomendame peliculas sobre enamoramientos\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "BOT: Aquí tienes una lista de películas románticas\n",
            "y alguna pelicula sobre investigación de un hecho delictivo?\n",
            "Q: y alguna pelicula sobre investigación de un hecho delictivo?\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "BOT: Claro , puedo recomendarte peliculas de acción\n",
            "Y una pelicula sobre conflictos armados entre naciones?\n",
            "Q: Y una pelicula sobre conflictos armados entre naciones?\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "BOT: Puedo recomendarte una película de suspenso\n",
            "No, una película sobre conflictos armados\n",
            "Q: No, una película sobre conflictos armados\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "BOT: Claro , puedo recomendarte peliculas de acción\n",
            "Sobre guerras\n",
            "Q: Sobre guerras\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "BOT: Aquí te comparto un listado de películas bélicas <link>\n",
            "Para todas las edades?\n",
            "Q: Para todas las edades?\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "BOT: Las peliculas que te recomiendo son aptas para todo público\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-136-369632efe651>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# pedir input al usuario\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Q:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "9W-ervE9FSQ3",
        "outputId": "e951218b-d8de-43d2-8e20-234a0930b5a0"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-135-26825c69cce8>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mwordvectors_file_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'fasttext-sbwc.3.6.e20.vec'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcantidad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mwordvectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordvectors_file_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcantidad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[1;32m   1717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m         \"\"\"\n\u001b[0;32m-> 1719\u001b[0;31m         return _load_word2vec_format(\n\u001b[0m\u001b[1;32m   1720\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1721\u001b[0m             \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_header\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mno_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[1;32m   2046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2047\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading projection weights from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2048\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2049\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mno_header\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2050\u001b[0m             \u001b[0;31m# deduce both vocab_size & vector_size from 1st pass over file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, compression, transport_params)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mtransport_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m     fobj = _shortcut_open(\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0mopen_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'errors'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'fasttext-sbwc.3.6.e20.vec'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayatkwp4fYQx"
      },
      "source": [
        "### 7 - Conclusiones\n",
        "El bot puede responder a preguntas si y solo si se utilizan dentro de las preguntas las palabras clave. Esto se puso de manifiesto al solicitar la recomendación de una comedia debiendo repetir la pregunta hasta usar la palabra \"graciosa\" que si estaba dentro de los patterns.El modelo es simple y no puede capturar relaciones latentes entre palabras (sinónimos, por ejemplo). Supongo que usando words embeddings la performance deberia mejorar.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}