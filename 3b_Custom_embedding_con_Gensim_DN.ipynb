{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZd5yLnnHOK0"
      },
      "source": [
        "<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n",
        "\n",
        "\n",
        "# Procesamiento de lenguaje natural\n",
        "## Custom embedddings con Gensim\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vA7nqkumo9z9"
      },
      "source": [
        "### Objetivo\n",
        " En este desafío voy a crear y entrenar los embeddings correspondientes a un corpus elaborado en base al texto de Sigmund Freud \"The interpretations of the dreams\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFToQs5FK5uZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import multiprocessing\n",
        "from gensim.models import Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g07zJxG7H9vG"
      },
      "source": [
        "### Datos\n",
        "Utilizaremos como dataset canciones de bandas de habla inglesa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 784
        },
        "id": "ticoqYD1Z3I7",
        "outputId": "bfdbf548-17f2-49b3-cf06-f26f2cf60157"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                Oración\n",
              "0     DO not know how familiar some of you may be, e...\n",
              "1      But, in keeping with the title of these lectu...\n",
              "2     \\n\\nTo be sure, this much I may presume that y...\n",
              "3      And just at this point I can give you an exam...\n",
              "4      Usually when we introduce a patient to a medi...\n",
              "...                                                 ...\n",
              "6128   I have undertaken to inform you concerning an...\n",
              "6129   Often I presented the evidence and then did n...\n",
              "6130   But I could not endeavor to make you masters ...\n",
              "6131   I tried only to give you some explanation and...\n",
              "6132                                                   \n",
              "\n",
              "[6133 rows x 1 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-df1afa51-fad9-4ad3-a100-0b216e698cd2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Oración</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>DO not know how familiar some of you may be, e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>But, in keeping with the title of these lectu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\\n\\nTo be sure, this much I may presume that y...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>And just at this point I can give you an exam...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Usually when we introduce a patient to a medi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6128</th>\n",
              "      <td>I have undertaken to inform you concerning an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6129</th>\n",
              "      <td>Often I presented the evidence and then did n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6130</th>\n",
              "      <td>But I could not endeavor to make you masters ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6131</th>\n",
              "      <td>I tried only to give you some explanation and...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6132</th>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6133 rows × 1 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-df1afa51-fad9-4ad3-a100-0b216e698cd2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-df1afa51-fad9-4ad3-a100-0b216e698cd2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-df1afa51-fad9-4ad3-a100-0b216e698cd2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-9c53d9f5-cb8d-466e-a31b-34840ad793d7\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9c53d9f5-cb8d-466e-a31b-34840ad793d7')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-9c53d9f5-cb8d-466e-a31b-34840ad793d7 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "# Monta Google Drive en tu entorno de Google Colab\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Ruta al archivo de texto en tu Google Drive\n",
        "file_path ='/content/drive/My Drive/The interpretation of the dreams.txt'\n",
        "\n",
        "# Lee el contenido del archivo de texto\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Divide el texto en oraciones (asumiendo que cada oración está separada por un punto)\n",
        "sentences = text.split('.')\n",
        "\n",
        "# Crea un DataFrame con las oraciones como filas\n",
        "df = pd.DataFrame(sentences, columns=['Oración'])\n",
        "\n",
        "# Muestra las primeras filas del DataFrame\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEpKubK9XzXN",
        "outputId": "9e1b15ba-ab36-43a6-afd2-f1f9b13d1b95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cantidad de documentos: 6133\n"
          ]
        }
      ],
      "source": [
        "print(\"Cantidad de documentos:\", df.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab94qaFlrA1G"
      },
      "source": [
        "### 1 - Preprocesamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIsmMWmjrDHd"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "\n",
        "sentence_tokens = []\n",
        "# Recorrer todas las filas y transformar las oraciones\n",
        "# en una secuencia de palabras (esto podría realizarse con NLTK o spaCy también)\n",
        "for _, row in df[:None].iterrows():\n",
        "    sentence_tokens.append(text_to_word_sequence(row[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHepi_DGrbhq",
        "outputId": "0e8cf9fa-0f6b-4866-c7fe-413e27637795"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['do',\n",
              "  'not',\n",
              "  'know',\n",
              "  'how',\n",
              "  'familiar',\n",
              "  'some',\n",
              "  'of',\n",
              "  'you',\n",
              "  'may',\n",
              "  'be',\n",
              "  'either',\n",
              "  'from',\n",
              "  'your',\n",
              "  'reading',\n",
              "  'or',\n",
              "  'from',\n",
              "  'hearsay',\n",
              "  'with',\n",
              "  'psychoanalysis'],\n",
              " ['but',\n",
              "  'in',\n",
              "  'keeping',\n",
              "  'with',\n",
              "  'the',\n",
              "  'title',\n",
              "  'of',\n",
              "  'these',\n",
              "  'lectures—a',\n",
              "  'general',\n",
              "  'introduction',\n",
              "  'to',\n",
              "  'psychoanalysis—i',\n",
              "  'am',\n",
              "  'obliged',\n",
              "  'to',\n",
              "  'proceed',\n",
              "  'as',\n",
              "  'though',\n",
              "  'you',\n",
              "  'knew',\n",
              "  'nothing',\n",
              "  'about',\n",
              "  'this',\n",
              "  'subject',\n",
              "  'and',\n",
              "  'stood',\n",
              "  'in',\n",
              "  'need',\n",
              "  'of',\n",
              "  'preliminary',\n",
              "  'instruction']]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# Demos un vistazo\n",
        "sentence_tokens[:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaXV6nlHr5Aa"
      },
      "source": [
        "### 2 - Crear los vectores (word2vec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSb0v7h8r7hK"
      },
      "outputs": [],
      "source": [
        "from gensim.models.callbacks import CallbackAny2Vec\n",
        "# Durante el entrenamiento gensim por defecto no informa el \"loss\" en cada época\n",
        "# Sobrecargamos el callback para poder tener esta información\n",
        "class callback(CallbackAny2Vec):\n",
        "    \"\"\"\n",
        "    Callback to print loss after each epoch\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.epoch = 0\n",
        "\n",
        "    def on_epoch_end(self, model):\n",
        "        loss = model.get_latest_training_loss()\n",
        "        if self.epoch == 0:\n",
        "            print('Loss after epoch {}: {}'.format(self.epoch, loss))\n",
        "        else:\n",
        "            print('Loss after epoch {}: {}'.format(self.epoch, loss- self.loss_previous_step))\n",
        "        self.epoch += 1\n",
        "        self.loss_previous_step = loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0wnDdv9sJ47"
      },
      "outputs": [],
      "source": [
        "# Crearmos el modelo generador de vectores\n",
        "# En este caso utilizaremos la estructura modelo Skipgram\n",
        "w2v_model = Word2Vec(min_count=1,    # frecuencia mínima de palabra para incluirla en el vocabulario\n",
        "                     window=5,       # cant de palabras antes y desp de la predicha\n",
        "                     vector_size=300,       # dimensionalidad de los vectores\n",
        "                     negative=20,    # cantidad de negative samples... 0 es no se usa\n",
        "                     workers=1,      # si tienen más cores pueden cambiar este valor\n",
        "                     sg=1)           # modelo 0:CBOW  1:skipgram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lTt8wErsf17"
      },
      "outputs": [],
      "source": [
        "# Obtener el vocabulario con los tokens\n",
        "w2v_model.build_vocab(sentence_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNc9qt4os5AT",
        "outputId": "3b57df16-2c2b-4787-8d56-f03f4637a53d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cantidad de docs en el corpus: 6133\n"
          ]
        }
      ],
      "source": [
        "# Cantidad de filas/docs encontradas en el corpus\n",
        "print(\"Cantidad de docs en el corpus:\", w2v_model.corpus_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idw9cHF3tSMl",
        "outputId": "3f7a4d8f-e1c5-4901-fe49-5108174fe203"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cantidad de words distintas en el corpus: 9584\n"
          ]
        }
      ],
      "source": [
        "# Cantidad de words encontradas en el corpus\n",
        "print(\"Cantidad de words distintas en el corpus:\", len(w2v_model.wv.index_to_key))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fC9mZ8DPk-UC"
      },
      "source": [
        "### 3 - Entrenar embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSp-x0PAsq56",
        "outputId": "9008ee99-743a-4fc9-ed9b-d23b052995e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss after epoch 0: 2620398.0\n",
            "Loss after epoch 1: 1787890.5\n",
            "Loss after epoch 2: 1632622.5\n",
            "Loss after epoch 3: 1630325.0\n",
            "Loss after epoch 4: 1589439.0\n",
            "Loss after epoch 5: 1549399.0\n",
            "Loss after epoch 6: 1534843.0\n",
            "Loss after epoch 7: 1517812.0\n",
            "Loss after epoch 8: 1497096.0\n",
            "Loss after epoch 9: 1478609.0\n",
            "Loss after epoch 10: 1416720.0\n",
            "Loss after epoch 11: 1385646.0\n",
            "Loss after epoch 12: 1366528.0\n",
            "Loss after epoch 13: 1347110.0\n",
            "Loss after epoch 14: 1328936.0\n",
            "Loss after epoch 15: 1311144.0\n",
            "Loss after epoch 16: 1290188.0\n",
            "Loss after epoch 17: 1271154.0\n",
            "Loss after epoch 18: 1250730.0\n",
            "Loss after epoch 19: 1232670.0\n",
            "Loss after epoch 20: 1220646.0\n",
            "Loss after epoch 21: 1204484.0\n",
            "Loss after epoch 22: 1184278.0\n",
            "Loss after epoch 23: 1106564.0\n",
            "Loss after epoch 24: 1100232.0\n",
            "Loss after epoch 25: 1088480.0\n",
            "Loss after epoch 26: 1081936.0\n",
            "Loss after epoch 27: 1086808.0\n",
            "Loss after epoch 28: 1070732.0\n",
            "Loss after epoch 29: 1072624.0\n",
            "Loss after epoch 30: 1063172.0\n",
            "Loss after epoch 31: 1062788.0\n",
            "Loss after epoch 32: 1061596.0\n",
            "Loss after epoch 33: 1058560.0\n",
            "Loss after epoch 34: 1059880.0\n",
            "Loss after epoch 35: 1057772.0\n",
            "Loss after epoch 36: 1054696.0\n",
            "Loss after epoch 37: 1047932.0\n",
            "Loss after epoch 38: 1049768.0\n",
            "Loss after epoch 39: 1050708.0\n",
            "Loss after epoch 40: 1051860.0\n",
            "Loss after epoch 41: 1051684.0\n",
            "Loss after epoch 42: 1044556.0\n",
            "Loss after epoch 43: 1051532.0\n",
            "Loss after epoch 44: 1049912.0\n",
            "Loss after epoch 45: 1047180.0\n",
            "Loss after epoch 46: 1044812.0\n",
            "Loss after epoch 47: 1050940.0\n",
            "Loss after epoch 48: 1047316.0\n",
            "Loss after epoch 49: 1051488.0\n",
            "Loss after epoch 50: 1039544.0\n",
            "Loss after epoch 51: 1042104.0\n",
            "Loss after epoch 52: 1047556.0\n",
            "Loss after epoch 53: 1047224.0\n",
            "Loss after epoch 54: 716048.0\n",
            "Loss after epoch 55: 241232.0\n",
            "Loss after epoch 56: 239976.0\n",
            "Loss after epoch 57: 239936.0\n",
            "Loss after epoch 58: 238664.0\n",
            "Loss after epoch 59: 239040.0\n",
            "Loss after epoch 60: 235960.0\n",
            "Loss after epoch 61: 236736.0\n",
            "Loss after epoch 62: 236304.0\n",
            "Loss after epoch 63: 236392.0\n",
            "Loss after epoch 64: 236344.0\n",
            "Loss after epoch 65: 234016.0\n",
            "Loss after epoch 66: 234680.0\n",
            "Loss after epoch 67: 232664.0\n",
            "Loss after epoch 68: 232584.0\n",
            "Loss after epoch 69: 230104.0\n",
            "Loss after epoch 70: 230344.0\n",
            "Loss after epoch 71: 229904.0\n",
            "Loss after epoch 72: 229952.0\n",
            "Loss after epoch 73: 227968.0\n",
            "Loss after epoch 74: 227424.0\n",
            "Loss after epoch 75: 226968.0\n",
            "Loss after epoch 76: 227576.0\n",
            "Loss after epoch 77: 224384.0\n",
            "Loss after epoch 78: 228120.0\n",
            "Loss after epoch 79: 225400.0\n",
            "Loss after epoch 80: 223928.0\n",
            "Loss after epoch 81: 225792.0\n",
            "Loss after epoch 82: 223256.0\n",
            "Loss after epoch 83: 223192.0\n",
            "Loss after epoch 84: 222448.0\n",
            "Loss after epoch 85: 224112.0\n",
            "Loss after epoch 86: 221416.0\n",
            "Loss after epoch 87: 221056.0\n",
            "Loss after epoch 88: 219216.0\n",
            "Loss after epoch 89: 221720.0\n",
            "Loss after epoch 90: 218224.0\n",
            "Loss after epoch 91: 219032.0\n",
            "Loss after epoch 92: 216208.0\n",
            "Loss after epoch 93: 218752.0\n",
            "Loss after epoch 94: 218072.0\n",
            "Loss after epoch 95: 215216.0\n",
            "Loss after epoch 96: 215520.0\n",
            "Loss after epoch 97: 215608.0\n",
            "Loss after epoch 98: 215480.0\n",
            "Loss after epoch 99: 216864.0\n",
            "Loss after epoch 100: 212776.0\n",
            "Loss after epoch 101: 214208.0\n",
            "Loss after epoch 102: 213168.0\n",
            "Loss after epoch 103: 210752.0\n",
            "Loss after epoch 104: 211248.0\n",
            "Loss after epoch 105: 211152.0\n",
            "Loss after epoch 106: 210912.0\n",
            "Loss after epoch 107: 210992.0\n",
            "Loss after epoch 108: 209888.0\n",
            "Loss after epoch 109: 208672.0\n",
            "Loss after epoch 110: 206360.0\n",
            "Loss after epoch 111: 208456.0\n",
            "Loss after epoch 112: 208392.0\n",
            "Loss after epoch 113: 208096.0\n",
            "Loss after epoch 114: 205472.0\n",
            "Loss after epoch 115: 206536.0\n",
            "Loss after epoch 116: 206424.0\n",
            "Loss after epoch 117: 204816.0\n",
            "Loss after epoch 118: 205160.0\n",
            "Loss after epoch 119: 205280.0\n",
            "Loss after epoch 120: 204168.0\n",
            "Loss after epoch 121: 203552.0\n",
            "Loss after epoch 122: 205568.0\n",
            "Loss after epoch 123: 202080.0\n",
            "Loss after epoch 124: 204296.0\n",
            "Loss after epoch 125: 202104.0\n",
            "Loss after epoch 126: 201976.0\n",
            "Loss after epoch 127: 199472.0\n",
            "Loss after epoch 128: 199720.0\n",
            "Loss after epoch 129: 198208.0\n",
            "Loss after epoch 130: 199704.0\n",
            "Loss after epoch 131: 201400.0\n",
            "Loss after epoch 132: 198432.0\n",
            "Loss after epoch 133: 197248.0\n",
            "Loss after epoch 134: 198888.0\n",
            "Loss after epoch 135: 196608.0\n",
            "Loss after epoch 136: 197928.0\n",
            "Loss after epoch 137: 193832.0\n",
            "Loss after epoch 138: 195192.0\n",
            "Loss after epoch 139: 199144.0\n",
            "Loss after epoch 140: 195168.0\n",
            "Loss after epoch 141: 194568.0\n",
            "Loss after epoch 142: 193560.0\n",
            "Loss after epoch 143: 193424.0\n",
            "Loss after epoch 144: 191936.0\n",
            "Loss after epoch 145: 191656.0\n",
            "Loss after epoch 146: 191088.0\n",
            "Loss after epoch 147: 191360.0\n",
            "Loss after epoch 148: 188768.0\n",
            "Loss after epoch 149: 190048.0\n",
            "Loss after epoch 150: 188792.0\n",
            "Loss after epoch 151: 189696.0\n",
            "Loss after epoch 152: 190568.0\n",
            "Loss after epoch 153: 188216.0\n",
            "Loss after epoch 154: 187720.0\n",
            "Loss after epoch 155: 186312.0\n",
            "Loss after epoch 156: 187560.0\n",
            "Loss after epoch 157: 186528.0\n",
            "Loss after epoch 158: 185320.0\n",
            "Loss after epoch 159: 186528.0\n",
            "Loss after epoch 160: 183256.0\n",
            "Loss after epoch 161: 183488.0\n",
            "Loss after epoch 162: 183816.0\n",
            "Loss after epoch 163: 185360.0\n",
            "Loss after epoch 164: 185136.0\n",
            "Loss after epoch 165: 182656.0\n",
            "Loss after epoch 166: 182272.0\n",
            "Loss after epoch 167: 184128.0\n",
            "Loss after epoch 168: 182472.0\n",
            "Loss after epoch 169: 180472.0\n",
            "Loss after epoch 170: 181080.0\n",
            "Loss after epoch 171: 180488.0\n",
            "Loss after epoch 172: 179104.0\n",
            "Loss after epoch 173: 177672.0\n",
            "Loss after epoch 174: 179680.0\n",
            "Loss after epoch 175: 179296.0\n",
            "Loss after epoch 176: 178336.0\n",
            "Loss after epoch 177: 178048.0\n",
            "Loss after epoch 178: 175872.0\n",
            "Loss after epoch 179: 176320.0\n",
            "Loss after epoch 180: 175416.0\n",
            "Loss after epoch 181: 174544.0\n",
            "Loss after epoch 182: 175192.0\n",
            "Loss after epoch 183: 175400.0\n",
            "Loss after epoch 184: 174064.0\n",
            "Loss after epoch 185: 173032.0\n",
            "Loss after epoch 186: 172088.0\n",
            "Loss after epoch 187: 172696.0\n",
            "Loss after epoch 188: 173336.0\n",
            "Loss after epoch 189: 169912.0\n",
            "Loss after epoch 190: 169808.0\n",
            "Loss after epoch 191: 169368.0\n",
            "Loss after epoch 192: 171648.0\n",
            "Loss after epoch 193: 169200.0\n",
            "Loss after epoch 194: 168512.0\n",
            "Loss after epoch 195: 168392.0\n",
            "Loss after epoch 196: 168552.0\n",
            "Loss after epoch 197: 168408.0\n",
            "Loss after epoch 198: 169936.0\n",
            "Loss after epoch 199: 166984.0\n",
            "Loss after epoch 200: 166632.0\n",
            "Loss after epoch 201: 165816.0\n",
            "Loss after epoch 202: 165080.0\n",
            "Loss after epoch 203: 165200.0\n",
            "Loss after epoch 204: 166112.0\n",
            "Loss after epoch 205: 163168.0\n",
            "Loss after epoch 206: 162744.0\n",
            "Loss after epoch 207: 164056.0\n",
            "Loss after epoch 208: 162776.0\n",
            "Loss after epoch 209: 161816.0\n",
            "Loss after epoch 210: 161360.0\n",
            "Loss after epoch 211: 161896.0\n",
            "Loss after epoch 212: 162048.0\n",
            "Loss after epoch 213: 160320.0\n",
            "Loss after epoch 214: 162984.0\n",
            "Loss after epoch 215: 160344.0\n",
            "Loss after epoch 216: 158976.0\n",
            "Loss after epoch 217: 159800.0\n",
            "Loss after epoch 218: 158080.0\n",
            "Loss after epoch 219: 156104.0\n",
            "Loss after epoch 220: 157584.0\n",
            "Loss after epoch 221: 157456.0\n",
            "Loss after epoch 222: 155328.0\n",
            "Loss after epoch 223: 155528.0\n",
            "Loss after epoch 224: 154144.0\n",
            "Loss after epoch 225: 156160.0\n",
            "Loss after epoch 226: 152840.0\n",
            "Loss after epoch 227: 155144.0\n",
            "Loss after epoch 228: 154784.0\n",
            "Loss after epoch 229: 151792.0\n",
            "Loss after epoch 230: 150344.0\n",
            "Loss after epoch 231: 151680.0\n",
            "Loss after epoch 232: 151232.0\n",
            "Loss after epoch 233: 150872.0\n",
            "Loss after epoch 234: 150392.0\n",
            "Loss after epoch 235: 150184.0\n",
            "Loss after epoch 236: 148848.0\n",
            "Loss after epoch 237: 148024.0\n",
            "Loss after epoch 238: 148968.0\n",
            "Loss after epoch 239: 149808.0\n",
            "Loss after epoch 240: 146712.0\n",
            "Loss after epoch 241: 146496.0\n",
            "Loss after epoch 242: 147976.0\n",
            "Loss after epoch 243: 145792.0\n",
            "Loss after epoch 244: 145056.0\n",
            "Loss after epoch 245: 147600.0\n",
            "Loss after epoch 246: 144632.0\n",
            "Loss after epoch 247: 144344.0\n",
            "Loss after epoch 248: 143448.0\n",
            "Loss after epoch 249: 140848.0\n",
            "Loss after epoch 250: 141368.0\n",
            "Loss after epoch 251: 142760.0\n",
            "Loss after epoch 252: 140928.0\n",
            "Loss after epoch 253: 141440.0\n",
            "Loss after epoch 254: 140616.0\n",
            "Loss after epoch 255: 140928.0\n",
            "Loss after epoch 256: 141720.0\n",
            "Loss after epoch 257: 139648.0\n",
            "Loss after epoch 258: 138760.0\n",
            "Loss after epoch 259: 137584.0\n",
            "Loss after epoch 260: 136976.0\n",
            "Loss after epoch 261: 135832.0\n",
            "Loss after epoch 262: 136720.0\n",
            "Loss after epoch 263: 134688.0\n",
            "Loss after epoch 264: 135824.0\n",
            "Loss after epoch 265: 138040.0\n",
            "Loss after epoch 266: 133568.0\n",
            "Loss after epoch 267: 134304.0\n",
            "Loss after epoch 268: 132912.0\n",
            "Loss after epoch 269: 134400.0\n",
            "Loss after epoch 270: 133384.0\n",
            "Loss after epoch 271: 133000.0\n",
            "Loss after epoch 272: 130048.0\n",
            "Loss after epoch 273: 132240.0\n",
            "Loss after epoch 274: 131648.0\n",
            "Loss after epoch 275: 131048.0\n",
            "Loss after epoch 276: 128856.0\n",
            "Loss after epoch 277: 130128.0\n",
            "Loss after epoch 278: 128352.0\n",
            "Loss after epoch 279: 128696.0\n",
            "Loss after epoch 280: 127760.0\n",
            "Loss after epoch 281: 126976.0\n",
            "Loss after epoch 282: 127520.0\n",
            "Loss after epoch 283: 127136.0\n",
            "Loss after epoch 284: 126888.0\n",
            "Loss after epoch 285: 124320.0\n",
            "Loss after epoch 286: 126312.0\n",
            "Loss after epoch 287: 125480.0\n",
            "Loss after epoch 288: 123592.0\n",
            "Loss after epoch 289: 126064.0\n",
            "Loss after epoch 290: 120544.0\n",
            "Loss after epoch 291: 123336.0\n",
            "Loss after epoch 292: 120656.0\n",
            "Loss after epoch 293: 121536.0\n",
            "Loss after epoch 294: 121400.0\n",
            "Loss after epoch 295: 121408.0\n",
            "Loss after epoch 296: 119352.0\n",
            "Loss after epoch 297: 118856.0\n",
            "Loss after epoch 298: 120984.0\n",
            "Loss after epoch 299: 117624.0\n",
            "Loss after epoch 300: 118224.0\n",
            "Loss after epoch 301: 118792.0\n",
            "Loss after epoch 302: 118696.0\n",
            "Loss after epoch 303: 116632.0\n",
            "Loss after epoch 304: 117032.0\n",
            "Loss after epoch 305: 115616.0\n",
            "Loss after epoch 306: 114368.0\n",
            "Loss after epoch 307: 114424.0\n",
            "Loss after epoch 308: 114888.0\n",
            "Loss after epoch 309: 114616.0\n",
            "Loss after epoch 310: 113528.0\n",
            "Loss after epoch 311: 113280.0\n",
            "Loss after epoch 312: 111040.0\n",
            "Loss after epoch 313: 113320.0\n",
            "Loss after epoch 314: 108120.0\n",
            "Loss after epoch 315: 109336.0\n",
            "Loss after epoch 316: 109216.0\n",
            "Loss after epoch 317: 108800.0\n",
            "Loss after epoch 318: 110624.0\n",
            "Loss after epoch 319: 108768.0\n",
            "Loss after epoch 320: 108192.0\n",
            "Loss after epoch 321: 106496.0\n",
            "Loss after epoch 322: 106624.0\n",
            "Loss after epoch 323: 106040.0\n",
            "Loss after epoch 324: 106496.0\n",
            "Loss after epoch 325: 106672.0\n",
            "Loss after epoch 326: 104176.0\n",
            "Loss after epoch 327: 104080.0\n",
            "Loss after epoch 328: 103744.0\n",
            "Loss after epoch 329: 103864.0\n",
            "Loss after epoch 330: 102104.0\n",
            "Loss after epoch 331: 101656.0\n",
            "Loss after epoch 332: 102776.0\n",
            "Loss after epoch 333: 101904.0\n",
            "Loss after epoch 334: 102600.0\n",
            "Loss after epoch 335: 101216.0\n",
            "Loss after epoch 336: 101296.0\n",
            "Loss after epoch 337: 100992.0\n",
            "Loss after epoch 338: 99800.0\n",
            "Loss after epoch 339: 98784.0\n",
            "Loss after epoch 340: 99976.0\n",
            "Loss after epoch 341: 98360.0\n",
            "Loss after epoch 342: 96704.0\n",
            "Loss after epoch 343: 98656.0\n",
            "Loss after epoch 344: 95720.0\n",
            "Loss after epoch 345: 98088.0\n",
            "Loss after epoch 346: 96416.0\n",
            "Loss after epoch 347: 97864.0\n",
            "Loss after epoch 348: 91616.0\n",
            "Loss after epoch 349: 93904.0\n",
            "Loss after epoch 350: 94512.0\n",
            "Loss after epoch 351: 94664.0\n",
            "Loss after epoch 352: 93864.0\n",
            "Loss after epoch 353: 91784.0\n",
            "Loss after epoch 354: 92104.0\n",
            "Loss after epoch 355: 89888.0\n",
            "Loss after epoch 356: 89480.0\n",
            "Loss after epoch 357: 88496.0\n",
            "Loss after epoch 358: 90792.0\n",
            "Loss after epoch 359: 91648.0\n",
            "Loss after epoch 360: 89848.0\n",
            "Loss after epoch 361: 88528.0\n",
            "Loss after epoch 362: 86576.0\n",
            "Loss after epoch 363: 88096.0\n",
            "Loss after epoch 364: 88208.0\n",
            "Loss after epoch 365: 85864.0\n",
            "Loss after epoch 366: 86872.0\n",
            "Loss after epoch 367: 84776.0\n",
            "Loss after epoch 368: 85504.0\n",
            "Loss after epoch 369: 84800.0\n",
            "Loss after epoch 370: 84872.0\n",
            "Loss after epoch 371: 85344.0\n",
            "Loss after epoch 372: 85664.0\n",
            "Loss after epoch 373: 84128.0\n",
            "Loss after epoch 374: 84232.0\n",
            "Loss after epoch 375: 84152.0\n",
            "Loss after epoch 376: 81232.0\n",
            "Loss after epoch 377: 80696.0\n",
            "Loss after epoch 378: 81352.0\n",
            "Loss after epoch 379: 79584.0\n",
            "Loss after epoch 380: 79320.0\n",
            "Loss after epoch 381: 80336.0\n",
            "Loss after epoch 382: 79168.0\n",
            "Loss after epoch 383: 79672.0\n",
            "Loss after epoch 384: 79464.0\n",
            "Loss after epoch 385: 78832.0\n",
            "Loss after epoch 386: 78120.0\n",
            "Loss after epoch 387: 77480.0\n",
            "Loss after epoch 388: 77544.0\n",
            "Loss after epoch 389: 79760.0\n",
            "Loss after epoch 390: 77904.0\n",
            "Loss after epoch 391: 76312.0\n",
            "Loss after epoch 392: 77320.0\n",
            "Loss after epoch 393: 75096.0\n",
            "Loss after epoch 394: 74472.0\n",
            "Loss after epoch 395: 75680.0\n",
            "Loss after epoch 396: 74768.0\n",
            "Loss after epoch 397: 74688.0\n",
            "Loss after epoch 398: 75848.0\n",
            "Loss after epoch 399: 74560.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(44354303, 61878000)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "# Entrenamos el modelo generador de vectores\n",
        "# Utilizamos nuestro callback\n",
        "w2v_model.train(sentence_tokens,\n",
        "                 total_examples=w2v_model.corpus_count,\n",
        "                 epochs=400,\n",
        "                 compute_loss = True,\n",
        "                 callbacks=[callback()]\n",
        "                 )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddT9NVuNlCAe"
      },
      "source": [
        "### 4 - Ensayar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cHN9xGLuPEm",
        "outputId": "7e3070a7-dd39-49e9-ffc6-980b3a974a94"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('demanded', 0.5005157589912415),\n",
              " ('unattained', 0.4658283591270447),\n",
              " ('platonic', 0.44239506125450134)]"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ],
      "source": [
        "# Palabras que MÁS se relacionan con...:\n",
        "w2v_model.wv.most_similar(positive=[\"ideal\"], topn=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La palabra \"ideal\" se relaciona con las palabras demandado , inalcanzable y platónico lo cual es evidente y correcto."
      ],
      "metadata": {
        "id": "VQ8oVOUz93p7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47HiU5gdkdMq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "920194c9-b233-4a1f-b931-0dffc0d2f532"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('naecke', 0.07786783576011658),\n",
              " ('bölsche', 0.060305215418338776),\n",
              " ('play', 0.052931975573301315)]"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ],
      "source": [
        "# Palabras que MENOS se relacionan con...:\n",
        "w2v_model.wv.most_similar(negative=[\"dream\"], topn=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPLDPgzBmQXt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0589af1b-90a4-49b4-adae-082cc461eccb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('upshot', 0.46611151099205017),\n",
              " ('amnesias', 0.3730827271938324),\n",
              " ('vomit', 0.36020493507385254)]"
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ],
      "source": [
        "# Palabras que MÁS se relacionan con...:\n",
        "w2v_model.wv.most_similar(positive=[\"hysterical\"], topn=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La palabra histérica se relaciona con palabras como amnesia y vómito que son síntomas típicos de histeria para Freud."
      ],
      "metadata": {
        "id": "rlz2FOSV-Xkf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_g8UVWe6lFmh"
      },
      "source": [
        "### 5 - Visualizar agrupación de vectores"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uiWfbQN9-iik"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDxEVXAivjr9"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import IncrementalPCA\n",
        "from sklearn.manifold import TSNE\n",
        "import numpy as np\n",
        "\n",
        "def reduce_dimensions(model, num_dimensions = 2 ):\n",
        "\n",
        "    vectors = np.asarray(model.wv.vectors)\n",
        "    labels = np.asarray(model.wv.index_to_key)\n",
        "\n",
        "    tsne = TSNE(n_components=num_dimensions, random_state=0)\n",
        "    vectors = tsne.fit_transform(vectors)\n",
        "\n",
        "    return vectors, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCCXtDpcugmd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "5e70d52d-d29c-4e4a-90ce-a2720648bea0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"f53844d9-a0bf-478e-93c5-56c9b377ad33\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"f53844d9-a0bf-478e-93c5-56c9b377ad33\")) {                    Plotly.newPlot(                        \"f53844d9-a0bf-478e-93c5-56c9b377ad33\",                        [{\"hovertemplate\":\"x=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003etext=%{text}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers+text\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"text\":[\"the\",\"of\",\"to\",\"in\",\"and\",\"a\",\"that\",\"is\",\"it\",\"we\",\"which\",\"this\",\"as\",\"not\",\"are\",\"for\",\"be\",\"with\",\"dream\",\"have\",\"you\",\"by\",\"i\",\"but\",\"he\",\"from\",\"one\",\"an\",\"or\",\"his\",\"has\",\"these\",\"at\",\"our\",\"can\",\"all\",\"on\",\"so\",\"will\",\"its\",\"was\",\"they\",\"may\",\"only\",\"no\",\"if\",\"other\",\"her\",\"us\",\"such\",\"sexual\",\"had\",\"must\",\"their\",\"more\",\"there\",\"into\",\"what\",\"do\",\"she\",\"when\",\"who\",\"been\",\"would\",\"dreams\",\"them\",\"very\",\"time\",\"him\",\"same\",\"life\",\"first\",\"then\",\"most\",\"psychic\",\"libido\",\"also\",\"know\",\"does\",\"fact\",\"even\",\"wish\",\"two\",\"out\",\"now\",\"patient\",\"were\",\"upon\",\"between\",\"those\",\"way\",\"cannot\",\"symptoms\",\"itself\",\"unconscious\",\"case\",\"work\",\"certain\",\"say\",\"up\",\"how\",\"fear\",\"another\",\"ego\",\"interpretation\",\"psychoanalysis\",\"through\",\"without\",\"some\",\"meaning\",\"well\",\"part\",\"me\",\"much\",\"neuroses\",\"something\",\"than\",\"means\",\"any\",\"could\",\"cases\",\"development\",\"shall\",\"many\",\"however\",\"my\",\"analysis\",\"child\",\"too\",\"about\",\"place\",\"himself\",\"made\",\"let\",\"object\",\"conditions\",\"point\",\"because\",\"under\",\"should\",\"become\",\"here\",\"man\",\"again\",\"after\",\"far\",\"just\",\"later\",\"before\",\"against\",\"therefore\",\"perhaps\",\"new\",\"give\",\"neurosis\",\"little\",\"often\",\"sleep\",\"see\",\"find\",\"yet\",\"day\",\"still\",\"nothing\",\"your\",\"really\",\"content\",\"person\",\"make\",\"take\",\"further\",\"like\",\"course\",\"own\",\"able\",\"every\",\"why\",\"might\",\"experience\",\"condition\",\"dreamer\",\"since\",\"symptom\",\"thus\",\"thought\",\"latent\",\"things\",\"neurotic\",\"thing\",\"already\",\"result\",\"example\",\"understand\",\"name\",\"being\",\"possible\",\"come\",\"present\",\"always\",\"persons\"],\"x\":[-15.21423625946045,-15.222469329833984,-16.060977935791016,-15.337117195129395,-15.207681655883789,-15.709564208984375,-16.21207618713379,-15.66724681854248,-16.11871910095215,-16.214691162109375,-15.181416511535645,-16.378753662109375,-16.18775177001953,-16.67549705505371,-15.583456039428711,-15.748032569885254,-16.616689682006836,-14.935166358947754,-15.973892211914062,-16.241256713867188,-16.677467346191406,-15.412226676940918,-20.49578285217285,-16.848726272583008,-17.00905418395996,-14.697896003723145,-16.439069747924805,-15.024517059326172,-14.417309761047363,-14.083569526672363,-15.12304401397705,-15.092229843139648,-16.0434627532959,-15.802797317504883,-16.834897994995117,-14.996014595031738,-15.288576126098633,-14.633166313171387,-16.972625732421875,-14.339652061462402,-16.781124114990234,-15.564260482788086,-17.149028778076172,-16.367372512817383,-22.66161346435547,-17.439716339111328,-14.315963745117188,-23.244544982910156,-20.561140060424805,-15.257675170898438,-14.635846138000488,-16.68878746032715,-17.561574935913086,-13.924687385559082,-21.57648277282715,-12.264779090881348,-14.07820987701416,-13.274698257446289,-17.304656982421875,-22.440473556518555,-14.692817687988281,-12.486953735351562,-17.223356246948242,-17.555198669433594,-16.203445434570312,-16.411396026611328,-15.75843334197998,-17.410823822021484,-12.833102226257324,-13.55449390411377,-14.191305160522461,-15.238856315612793,-13.871254920959473,-13.923338890075684,-13.572360038757324,-20.238887786865234,-14.414816856384277,-16.775400161743164,-17.830320358276367,-17.913015365600586,-17.021957397460938,-17.230331420898438,-14.427347183227539,-19.38040542602539,-14.349117279052734,-14.502689361572266,-15.891829490661621,-12.412531852722168,-15.678210258483887,-14.96728229522705,-15.070282936096191,-18.020761489868164,-16.23224449157715,-13.280014038085938,-14.128116607666016,-18.72602081298828,-15.529443740844727,-13.423307418823242,-12.96191120147705,-18.292644500732422,-17.33147621154785,-13.20464038848877,-22.819395065307617,-11.535950660705566,-17.258466720581055,-17.798294067382812,-14.382925033569336,-11.4948091506958,-17.688617706298828,-16.721580505371094,-18.182279586791992,-12.995018005371094,-20.74478530883789,-13.115660667419434,-17.005226135253906,-18.173616409301758,-21.747589111328125,-16.18941307067871,-15.784222602844238,-17.765073776245117,-13.628555297851562,-14.489348411560059,-17.08711051940918,-13.179332733154297,-16.916994094848633,-20.399707794189453,-19.013935089111328,-12.78964614868164,-17.883766174316406,-20.0517520904541,-18.92009162902832,-20.115581512451172,-15.978691101074219,-20.819799423217773,-14.334507942199707,-17.0379638671875,-22.935388565063477,-13.310822486877441,16.55339813232422,-16.639787673950195,-15.46572208404541,-12.352667808532715,-12.326160430908203,-13.409443855285645,-16.256303787231445,-13.598127365112305,-18.282363891601562,-18.882322311401367,-12.197092056274414,-12.679078102111816,-18.784971237182617,-19.72043800354004,-19.593353271484375,-21.748882293701172,-18.023590087890625,-18.822296142578125,-18.72275733947754,-11.539240837097168,-13.436005592346191,-17.797849655151367,-18.337894439697266,-17.68830108642578,-19.60984230041504,-18.877504348754883,-34.59840393066406,-11.290790557861328,-18.88067626953125,-14.437031745910645,-20.00293731689453,-16.315711975097656,-18.423320770263672,-23.992006301879883,-12.738009452819824,-17.506555557250977,-14.898298263549805,-18.778339385986328,-21.53982925415039,-19.341602325439453,-19.52505874633789,-11.884150505065918,-16.2081298828125,-14.335139274597168,-13.856758117675781,-11.584630012512207,-15.101778030395508,-10.446660995483398,-20.013830184936523,-12.358238220214844,-13.762554168701172,-19.529142379760742,-11.8610258102417,-10.576396942138672,-18.107666015625,-13.821126937866211,-27.774328231811523,-19.50983238220215,-28.50244140625,-19.530075073242188,-20.627521514892578,-13.668869018554688],\"xaxis\":\"x\",\"y\":[4.305973529815674,4.376104354858398,3.987891435623169,4.350252628326416,4.202947616577148,4.798275947570801,4.637632369995117,4.899636268615723,4.685131549835205,3.6102354526519775,4.882308483123779,4.669999599456787,4.996313571929932,4.101146221160889,3.5245232582092285,5.253420829772949,4.218064785003662,4.489901065826416,5.936604022979736,3.8010644912719727,3.168584108352661,5.399971008300781,2.7535154819488525,4.771514415740967,5.18636417388916,5.2089619636535645,5.321268081665039,5.281429767608643,4.799690246582031,3.696545362472534,6.25920295715332,3.507363796234131,3.2186453342437744,2.9873876571655273,3.7950549125671387,3.6272952556610107,2.612750291824341,3.5527231693267822,3.415294647216797,4.407696723937988,6.222403526306152,3.317037582397461,4.327760696411133,5.740626811981201,12.255289077758789,5.282957077026367,5.519996166229248,4.905051231384277,7.108288288116455,5.884632587432861,5.86901330947876,6.790744781494141,4.7579264640808105,4.213514804840088,5.728898525238037,3.7841289043426514,5.000550746917725,2.0313985347747803,3.0863358974456787,4.885241985321045,6.691565036773682,3.3309481143951416,6.95239782333374,3.857149600982666,6.583418846130371,2.6537413597106934,6.10936164855957,6.089656352996826,0.7813278436660767,5.493199825286865,6.395015239715576,6.855998516082764,3.2548420429229736,6.010782241821289,4.880289554595947,5.536985874176025,4.002986907958984,2.672978401184082,4.231138706207275,5.626704216003418,5.774387836456299,14.448182106018066,10.241292953491211,5.635329723358154,2.341280460357666,3.1167991161346436,2.064387083053589,0.37558069825172424,8.380640983581543,2.8022115230560303,2.4009275436401367,4.683731555938721,7.3784589767456055,7.487541675567627,2.758816957473755,6.201736927032471,7.576355457305908,4.4915876388549805,1.3876911401748657,3.011845588684082,2.3972153663635254,3.627847909927368,3.8308897018432617,6.1520795822143555,8.096553802490234,7.567371368408203,7.552187442779541,4.396951198577881,6.632232189178467,8.364786148071289,6.202415466308594,6.741235256195068,1.7869389057159424,5.472296714782715,10.649728775024414,7.029910087585449,5.775542736053467,7.983810901641846,6.882650375366211,3.507115125656128,6.929415225982666,9.141758918762207,1.8180791139602661,3.1135849952697754,7.488896369934082,2.9073708057403564,3.7759432792663574,4.394650936126709,2.8219635486602783,8.899907112121582,6.793979167938232,4.63458251953125,2.483276128768921,7.2264180183410645,7.130594730377197,9.732524871826172,8.958335876464844,3.960681438446045,-55.825130462646484,1.903947114944458,9.077187538146973,4.47062873840332,8.115690231323242,6.29304838180542,9.908687591552734,2.6920230388641357,5.365266799926758,5.4019880294799805,6.056723117828369,5.086470127105713,4.785924434661865,6.1180620193481445,4.928141117095947,4.59788179397583,8.164517402648926,8.121520042419434,4.371889591217041,6.850700378417969,1.3145326375961304,2.1558938026428223,3.7431576251983643,8.914454460144043,9.809504508972168,2.5025198459625244,-12.683927536010742,5.505802631378174,3.18896222114563,1.934346079826355,3.804361581802368,1.5289016962051392,2.1208724975585938,0.3712112605571747,5.998049736022949,-5.004364013671875,1.3748619556427002,7.509350776672363,2.44954514503479,1.101248860359192,7.334195613861084,4.936114311218262,9.070088386535645,8.325305938720703,1.8396409749984741,1.8812174797058105,7.724348545074463,0.8181682825088501,1.6471868753433228,6.820849418640137,7.954974174499512,2.4623053073883057,7.41978645324707,8.92944049835205,0.6273619532585144,9.122125625610352,21.656190872192383,4.1034369468688965,-1.0842758417129517,3.209460735321045,3.986819267272949,-3.256486654281616],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"x\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"y\"}},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('f53844d9-a0bf-478e-93c5-56c9b377ad33');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Graficar los embedddings en 2D\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "\n",
        "vecs, labels = reduce_dimensions(w2v_model)\n",
        "\n",
        "MAX_WORDS=200\n",
        "fig = px.scatter(x=vecs[:MAX_WORDS,0], y=vecs[:MAX_WORDS,1], text=labels[:MAX_WORDS])\n",
        "fig.show(renderer=\"colab\") # esto para plotly en colab"
        "fig.write_image("newplot.png")",
        "Image("newplot.png")",
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se observa en el gráfico en 2D la cercanía en el espacio de embeddings de los vectores para los términos \"psychoanalysis\" \"symptoms\" \"interpretation\" \"meaning\" \"neurosis\" y \"dream\" lo cual es coherente dado que son palabras cardinales para el psicoanálisis y en particular para el texto usado de corpus."
      ],
      "metadata": {
        "id": "TDD-Uaq78-om"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Tf-inWvGx2g"
      },
      "outputs": [],
      "source": [
        "# También se pueden guardar los vectores y labels como tsv para graficar en\n",
        "# http://projector.tensorflow.org/\n",
        "\n",
        "vectors = np.asarray(w2v_model.wv.vectors)\n",
        "labels = list(w2v_model.wv.index_to_key)\n",
        "\n",
        "np.savetxt(\"vectors.tsv\", vectors, delimiter=\"\\t\")\n",
        "\n",
        "with open(\"labels.tsv\", \"w\") as fp:\n",
        "    for item in labels:\n",
        "        fp.write(\"%s\\n\" % item)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Test de analogías"
      ],
      "metadata": {
        "id": "wczUXbUIdI_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_matrix_distance(words, dist):\n",
        "    fig = plt.figure(figsize=(16,9))\n",
        "    ax = fig.add_subplot()\n",
        "    sns.heatmap(dist, xticklabels=words, yticklabels=words,\n",
        "                annot=True, fmt=\".2f\", cmap=\"YlGnBu\", ax=ax, mask=np.triu(dist))\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "rulfNuqfo1g4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import pairwise\n",
        "# probamos un test de analogía\n",
        "test_words = ['dream','unrecognizable','content']\n",
        "\n",
        "test_emb = [w2v_model.wv.get_vector(word) for word in test_words]\n",
        "\n",
        "test_word = test_emb[0] + test_emb[1] + test_emb[2]\n",
        "\n",
        "new_words = w2v_model.wv.most_similar(positive=[test_word], topn=5)\n",
        "\n",
        "new_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kcrk_DPdIAa",
        "outputId": "16929777-6c5e-43d9-f6a2-a2c7075ed5c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('unrecognizable', 0.7707424163818359),\n",
              " ('content', 0.6833397746086121),\n",
              " ('dream', 0.5701570510864258),\n",
              " ('unfitting', 0.4503399431705475),\n",
              " ('genuine', 0.4501378834247589)]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Se observa que si combino las palabras \"dream\", \"unrecognizable\" y \"content\" las mayores similitudes resultantes dentro del corpus se obtienen con los términos \"genuine\" y \"unfitting\" lo cual representa el concepto de que durante el sueño los contenidos irreconocibles (por la conciencia del individuo) se  condensan en representaciones que manifiestan a esos contenidos genuinos pero inadecuados  (desde un punto de vista social o moral)."
      ],
      "metadata": {
        "id": "_8-BDV0zJghk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# probamos otro test de analogía\n",
        "test_words = ['dream','analysis']\n",
        "\n",
        "test_emb = [w2v_model.wv.get_vector(word) for word in test_words]\n",
        "\n",
        "\n",
        "test_word = test_emb[0] + test_emb[1]\n",
        "\n",
        "new_words = w2v_model.wv.most_similar(positive=[test_word], topn=3)\n",
        "\n",
        "new_words"
      ],
      "metadata": {
        "id": "_ucxPH27JdBR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2174f785-5597-4bb1-c366-013aaf067206"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('analysis', 0.8387126326560974),\n",
              " ('dream', 0.6251039505004883),\n",
              " ('reconnaissance', 0.3626308739185333)]"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Acá tambien se puede ver que la analogía tiene sentido en el contexto bajo estudio ya que si al vector que representa al \"sueño\" (dream) le sumo \"analysis\" (análisis) la palabra con mayor similitud a la construcción es \"reconnaissance\" (reconocimiento) ya que el análisis de los sueños lleva a un reconocimiento de tendencias anímicas inconscientes."
      ],
      "metadata": {
        "id": "G-OUbBe0rtb2"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
